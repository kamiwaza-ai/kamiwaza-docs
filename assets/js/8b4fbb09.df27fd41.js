"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[1270],{5088:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"use-cases/building-a-rag-pipeline","title":"Building a RAG Pipeline","description":"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza\'s core services.","source":"@site/docs/use-cases/building-a-rag-pipeline.md","sourceDirName":"use-cases","slug":"/use-cases/building-a-rag-pipeline","permalink":"/kamiwaza-docs/use-cases/building-a-rag-pipeline","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"building-a-rag-pipeline","title":"Building a RAG Pipeline","sidebar_position":2},"sidebar":"mainSidebar","previous":{"title":"Use Cases","permalink":"/kamiwaza-docs/use-cases/"},"next":{"title":"Platform Overview","permalink":"/kamiwaza-docs/architecture/overview"}}');var i=s(4848),r=s(8453);const a={id:"building-a-rag-pipeline",title:"Building a RAG Pipeline",sidebar_position:2},o="Building a RAG Pipeline",l={},c=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Step 1: Deploy Required Models",id:"step-1-deploy-required-models",level:2},{value:"Deploy an Embedding Model",id:"deploy-an-embedding-model",level:3},{value:"Deploy a Language Model",id:"deploy-a-language-model",level:3},{value:"Step 2: Set Up Vector Storage",id:"step-2-set-up-vector-storage",level:2},{value:"Using the API",id:"using-the-api",level:3},{value:"Using the Kamiwaza SDK",id:"using-the-kamiwaza-sdk",level:3},{value:"Step 3: Document Ingestion Pipeline",id:"step-3-document-ingestion-pipeline",level:2},{value:"Document Processing Script",id:"document-processing-script",level:3},{value:"Step 4: Implement Retrieval and Generation",id:"step-4-implement-retrieval-and-generation",level:2},{value:"Step 5: Create a Simple Web Interface",id:"step-5-create-a-simple-web-interface",level:2},{value:"Step 6: Deploy as an App Garden Application",id:"step-6-deploy-as-an-app-garden-application",level:2},{value:"Create docker-compose.yml",id:"create-docker-composeyml",level:3},{value:"Create Dockerfile",id:"create-dockerfile",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Document Processing",id:"document-processing",level:3},{value:"Vector Search Optimization",id:"vector-search-optimization",level:3},{value:"LLM Integration",id:"llm-integration",level:3},{value:"Monitoring and Maintenance",id:"monitoring-and-maintenance",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Regular Updates",id:"regular-updates",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"building-a-rag-pipeline",children:"Building a RAG Pipeline"})}),"\n",(0,i.jsx)(n.p,{children:"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza's core services."}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this guide, you'll have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document ingestion system"})," that processes various file formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding pipeline"})," that converts text to vector representations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vector search system"})," for finding relevant context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM integration"})," that generates responses using retrieved context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Web interface"})," for querying your documents"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Kamiwaza installed and running (",(0,i.jsx)(n.a,{href:"../installation/installation_process",children:"Installation Guide"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"At least 8GB of available RAM"}),"\n",(0,i.jsx)(n.li,{children:"Sample documents (PDFs, text files, etc.) to process"}),"\n",(0,i.jsx)(n.li,{children:"Basic familiarity with Python (for SDK examples)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(n.p,{children:"A RAG pipeline consists of four main components:"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\n    A[Documents] --\x3e B[Ingestion & Chunking]\n    B --\x3e C[Embedding Generation]\n    C --\x3e D[Vector Storage]\n    D --\x3e E[Similarity Search]\n    E --\x3e F[LLM Generation]\n    F --\x3e G[Response]"}),"\n",(0,i.jsx)(n.h2,{id:"step-1-deploy-required-models",children:"Step 1: Deploy Required Models"}),"\n",(0,i.jsx)(n.p,{children:"First, we'll deploy an embedding model for vectorizing text and a language model for generating responses."}),"\n",(0,i.jsx)(n.h3,{id:"deploy-an-embedding-model",children:"Deploy an Embedding Model"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to the ",(0,i.jsx)(n.strong,{children:"Models"})," section in Kamiwaza"]}),"\n",(0,i.jsxs)(n.li,{children:["Search for ",(0,i.jsx)(n.code,{children:"sentence-transformers/all-MiniLM-L6-v2"})]}),"\n",(0,i.jsx)(n.li,{children:"Download and deploy the model"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Or use the API:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import requests\n\n# Deploy embedding model\nrequests.post('http://localhost:7777/api/models/sentence-transformers/all-MiniLM-L6-v2/deploy', json={\n    'engine': 'transformers',\n    'config': {\n        'task': 'feature-extraction'\n    }\n})\n"})}),"\n",(0,i.jsx)(n.h3,{id:"deploy-a-language-model",children:"Deploy a Language Model"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Search for and deploy ",(0,i.jsx)(n.code,{children:"Qwen/Qwen2.5-0.5B-Instruct-GGUF"})]}),"\n",(0,i.jsxs)(n.li,{children:["Select the ",(0,i.jsx)(n.code,{children:"Qwen2.5-0.5B-Instruct-Q6_K.gguf"})," file for CPU inference"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Deploy chat model\nrequests.post('http://localhost:7777/api/models/Qwen/Qwen2.5-0.5B-Instruct-GGUF/deploy', json={\n    'engine': 'llamacpp',\n    'config': {\n        'gpu_layers': 0,  # CPU inference\n        'context_size': 4096\n    }\n})\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-2-set-up-vector-storage",children:"Step 2: Set Up Vector Storage"}),"\n",(0,i.jsx)(n.p,{children:"Create a vector collection to store document embeddings."}),"\n",(0,i.jsx)(n.h3,{id:"using-the-api",children:"Using the API"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import requests\n\n# Create vector collection\ncollection_config = {\n    "name": "documents",\n    "dimension": 384,  # all-MiniLM-L6-v2 output dimension\n    "metric_type": "COSINE",\n    "index_type": "IVF_FLAT",\n    "metadata_schema": {\n        "title": "string",\n        "source": "string",\n        "chunk_id": "string",\n        "page": "integer"\n    }\n}\n\nresponse = requests.post(\n    \'http://localhost:7777/api/vectordb/collections\',\n    json=collection_config\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"using-the-kamiwaza-sdk",children:"Using the Kamiwaza SDK"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from kamiwaza_sdk import KamiwazaClient\n\nclient = KamiwazaClient(base_url="http://localhost:7777")\n\n# Create collection\nclient.vectordb.create_collection(\n    name="documents",\n    dimension=384,\n    metric_type="COSINE",\n    metadata_schema={\n        "title": "str",\n        "source": "str", \n        "chunk_id": "str",\n        "page": "int"\n    }\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-3-document-ingestion-pipeline",children:"Step 3: Document Ingestion Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Now we'll create a pipeline to process documents, chunk them, and generate embeddings."}),"\n",(0,i.jsx)(n.h3,{id:"document-processing-script",children:"Document Processing Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nimport requests\nfrom pathlib import Path\nfrom typing import List, Dict\nimport PyPDF2\nimport numpy as np\n\nclass RAGPipeline:\n    def __init__(self, base_url="http://localhost:7777"):\n        self.base_url = base_url\n        self.embedding_model = "sentence-transformers/all-MiniLM-L6-v2"\n        self.chat_model = "Qwen/Qwen2.5-0.5B-Instruct-GGUF"\n        \n    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n        """Extract text from PDF and return chunks with metadata."""\n        chunks = []\n        \n        with open(pdf_path, \'rb\') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            \n            for page_num, page in enumerate(pdf_reader.pages):\n                text = page.extract_text()\n                # Simple chunking - split by paragraphs\n                paragraphs = [p.strip() for p in text.split(\'\\n\\n\') if p.strip()]\n                \n                for chunk_id, paragraph in enumerate(paragraphs):\n                    if len(paragraph) > 100:  # Filter out very short chunks\n                        chunks.append({\n                            \'text\': paragraph,\n                            \'metadata\': {\n                                \'title\': Path(pdf_path).stem,\n                                \'source\': pdf_path,\n                                \'page\': page_num + 1,\n                                \'chunk_id\': f"{page_num}_{chunk_id}"\n                            }\n                        })\n        \n        return chunks\n    \n    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n        """Generate embeddings for a list of texts."""\n        response = requests.post(\n            f\'{self.base_url}/api/models/{self.embedding_model}/embeddings\',\n            json={\'input\': texts}\n        )\n        \n        if response.status_code == 200:\n            return response.json()[\'data\']\n        else:\n            raise Exception(f"Embedding generation failed: {response.text}")\n    \n    def store_vectors(self, chunks: List[Dict], embeddings: List[List[float]]):\n        """Store text chunks and their embeddings in the vector database."""\n        vectors_data = {\n            "vectors": [\n                {\n                    "id": f"{chunk[\'metadata\'][\'source\']}_{chunk[\'metadata\'][\'chunk_id\']}",\n                    "vector": embedding,\n                    "metadata": {\n                        **chunk[\'metadata\'],\n                        "text": chunk[\'text\']\n                    }\n                }\n                for chunk, embedding in zip(chunks, embeddings)\n            ]\n        }\n        \n        response = requests.post(\n            f\'{self.base_url}/api/vectordb/collections/documents/insert\',\n            json=vectors_data\n        )\n        \n        if response.status_code != 200:\n            raise Exception(f"Vector storage failed: {response.text}")\n    \n    def process_document(self, document_path: str):\n        """Complete pipeline to process a single document."""\n        print(f"Processing: {document_path}")\n        \n        # Extract text chunks\n        chunks = self.extract_text_from_pdf(document_path)\n        print(f"Extracted {len(chunks)} chunks")\n        \n        # Generate embeddings\n        texts = [chunk[\'text\'] for chunk in chunks]\n        embeddings = self.generate_embeddings(texts)\n        print(f"Generated {len(embeddings)} embeddings")\n        \n        # Store in vector database\n        self.store_vectors(chunks, embeddings)\n        print("Stored vectors successfully")\n\n# Usage\npipeline = RAGPipeline()\n\n# Process all PDFs in a directory\ndocs_directory = "path/to/your/documents"\nfor pdf_file in Path(docs_directory).glob("*.pdf"):\n    pipeline.process_document(str(pdf_file))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-4-implement-retrieval-and-generation",children:"Step 4: Implement Retrieval and Generation"}),"\n",(0,i.jsx)(n.p,{children:"Now we'll create the query interface that retrieves relevant documents and generates responses."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class RAGQuery:\n    def __init__(self, base_url=\"http://localhost:7777\"):\n        self.base_url = base_url\n        self.embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n        self.chat_model = \"Qwen/Qwen2.5-0.5B-Instruct-GGUF\"\n    \n    def embed_query(self, query: str) -> List[float]:\n        \"\"\"Generate embedding for the user query.\"\"\"\n        response = requests.post(\n            f'{self.base_url}/api/models/{self.embedding_model}/embeddings',\n            json={'input': [query]}\n        )\n        \n        if response.status_code == 200:\n            return response.json()['data'][0]\n        else:\n            raise Exception(f\"Query embedding failed: {response.text}\")\n    \n    def retrieve_context(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:\n        \"\"\"Retrieve relevant document chunks.\"\"\"\n        search_request = {\n            \"vector\": query_embedding,\n            \"top_k\": top_k,\n            \"include_metadata\": True\n        }\n        \n        response = requests.post(\n            f'{self.base_url}/api/vectordb/collections/documents/search',\n            json=search_request\n        )\n        \n        if response.status_code == 200:\n            return response.json()['results']\n        else:\n            raise Exception(f\"Vector search failed: {response.text}\")\n    \n    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:\n        \"\"\"Generate response using retrieved context.\"\"\"\n        # Prepare context from retrieved chunks\n        context = \"\\n\\n\".join([\n            f\"Document: {chunk['metadata']['title']} (Page {chunk['metadata']['page']})\\n{chunk['metadata']['text']}\"\n            for chunk in context_chunks\n        ])\n        \n        # Create prompt with context\n        prompt = f\"\"\"Based on the following context, answer the user's question. If the context doesn't contain enough information to answer the question, say so.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        # Generate response\n        response = requests.post(\n            f'{self.base_url}/api/models/{self.chat_model}/chat',\n            json={\n                'messages': [\n                    {'role': 'user', 'content': prompt}\n                ],\n                'max_tokens': 500,\n                'temperature': 0.7\n            }\n        )\n        \n        if response.status_code == 200:\n            return response.json()['choices'][0]['message']['content']\n        else:\n            raise Exception(f\"Response generation failed: {response.text}\")\n    \n    def query(self, user_question: str) -> Dict:\n        \"\"\"Complete RAG query pipeline.\"\"\"\n        print(f\"Processing query: {user_question}\")\n        \n        # Generate query embedding\n        query_embedding = self.embed_query(user_question)\n        \n        # Retrieve relevant context\n        context_chunks = self.retrieve_context(query_embedding)\n        \n        # Generate response\n        response = self.generate_response(user_question, context_chunks)\n        \n        return {\n            'question': user_question,\n            'answer': response,\n            'sources': [\n                {\n                    'title': chunk['metadata']['title'],\n                    'page': chunk['metadata']['page'],\n                    'score': chunk['score']\n                }\n                for chunk in context_chunks\n            ]\n        }\n\n# Usage\nrag = RAGQuery()\n\n# Query your documents\nresult = rag.query(\"What are the main benefits of artificial intelligence?\")\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Sources: {result['sources']}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-5-create-a-simple-web-interface",children:"Step 5: Create a Simple Web Interface"}),"\n",(0,i.jsx)(n.p,{children:"Let's create a basic web interface using Streamlit to interact with your RAG system."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import streamlit as st\nimport requests\nfrom rag_pipeline import RAGQuery  # Your RAG implementation\n\n# Initialize RAG system\n@st.cache_resource\ndef init_rag():\n    return RAGQuery()\n\nst.title("\ud83d\udcda Document Q&A with RAG")\nst.write("Ask questions about your uploaded documents!")\n\n# Initialize RAG\nrag = init_rag()\n\n# Query interface\nuser_question = st.text_input("Ask a question about your documents:")\n\nif user_question:\n    with st.spinner("Searching documents and generating answer..."):\n        try:\n            result = rag.query(user_question)\n            \n            # Display answer\n            st.success("Answer:")\n            st.write(result[\'answer\'])\n            \n            # Display sources\n            st.subheader("\ud83d\udcd6 Sources:")\n            for source in result[\'sources\']:\n                st.write(f"- **{source[\'title\']}** (Page {source[\'page\']}) - Relevance: {source[\'score\']:.3f}")\n                \n        except Exception as e:\n            st.error(f"Error: {e}")\n\n# Sidebar for system status\nst.sidebar.header("System Status")\ntry:\n    # Check if models are deployed\n    models_response = requests.get(\'http://localhost:7777/api/models\')\n    if models_response.status_code == 200:\n        st.sidebar.success("\u2705 Models service online")\n    else:\n        st.sidebar.error("\u274c Models service offline")\n        \n    # Check vector database\n    vectordb_response = requests.get(\'http://localhost:7777/api/vectordb/status\')\n    if vectordb_response.status_code == 200:\n        st.sidebar.success("\u2705 Vector database online")\n    else:\n        st.sidebar.error("\u274c Vector database offline")\n        \nexcept Exception as e:\n    st.sidebar.error(f"\u274c System check failed: {e}")\n'})}),"\n",(0,i.jsx)(n.p,{children:"Run the interface with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"streamlit run rag_interface.py\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-6-deploy-as-an-app-garden-application",children:"Step 6: Deploy as an App Garden Application"}),"\n",(0,i.jsx)(n.p,{children:"For production use, you can package your RAG system as an App Garden application."}),"\n",(0,i.jsx)(n.h3,{id:"create-docker-composeyml",children:"Create docker-compose.yml"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"version: '3.8'\n\nservices:\n  rag-app:\n    build: .\n    ports:\n      - \"8501:8501\"\n    environment:\n      - KAMIWAZA_BASE_URL=http://host.docker.internal:7777\n    volumes:\n      - ./documents:/app/documents\n    depends_on:\n      - kamiwaza\n\n  kamiwaza:\n    external: true\n"})}),"\n",(0,i.jsx)(n.h3,{id:"create-dockerfile",children:"Create Dockerfile"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'FROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8501\n\nCMD ["streamlit", "run", "rag_interface.py", "--server.address", "0.0.0.0"]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"document-processing",children:"Document Processing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chunk Size"}),": Keep chunks between 200-800 tokens for optimal retrieval"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Overlap"}),": Add 50-100 token overlap between chunks to preserve context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metadata"}),": Include rich metadata (source, date, author) for filtering"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preprocessing"}),": Clean text, remove headers/footers, handle special characters"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"vector-search-optimization",children:"Vector Search Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Index Tuning"}),": Adjust index parameters based on collection size"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reranking"}),": Use a reranking model for better result quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Search"}),": Combine vector search with keyword matching"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering"}),": Use metadata filters to narrow search scope"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Window"}),": Stay within model's context limits"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prompt Engineering"}),": Design clear, specific system prompts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temperature"}),": Use lower values (0.1-0.3) for factual responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Citations"}),": Always include source attribution in responses"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-maintenance",children:"Monitoring and Maintenance"}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Add timing and logging to your pipeline\nimport time\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef query_with_monitoring(self, query: str):\n    start_time = time.time()\n    \n    # Your query logic here\n    result = self.query(query)\n    \n    elapsed_time = time.time() - start_time\n    logger.info(f"Query processed in {elapsed_time:.2f}s")\n    \n    return result\n'})}),"\n",(0,i.jsx)(n.h3,{id:"regular-updates",children:"Regular Updates"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Refresh Embeddings"}),": Reprocess documents when updating embedding models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Index Optimization"}),": Rebuild vector indices periodically"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Updates"}),": Test and deploy newer, better models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Content Updates"}),": Set up automated document ingestion for new content"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Poor Retrieval Quality"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Check embedding model performance on your domain"}),"\n",(0,i.jsx)(n.li,{children:"Adjust chunk size and overlap"}),"\n",(0,i.jsx)(n.li,{children:"Try different similarity metrics (cosine vs. dot product)"}),"\n",(0,i.jsx)(n.li,{children:"Consider domain-specific fine-tuning"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Slow Query Performance"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimize vector index parameters"}),"\n",(0,i.jsxs)(n.li,{children:["Reduce ",(0,i.jsx)(n.code,{children:"top_k"})," in retrieval"]}),"\n",(0,i.jsx)(n.li,{children:"Use GPU acceleration for embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Implement result caching"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Inaccurate Responses"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improve prompt engineering"}),"\n",(0,i.jsx)(n.li,{children:"Increase retrieved context size"}),"\n",(0,i.jsx)(n.li,{children:"Use a larger/better language model"}),"\n",(0,i.jsx)(n.li,{children:"Add response validation logic"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Now that you have a working RAG pipeline:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Learn about advanced search techniques in the ",(0,i.jsx)(n.a,{href:"../data-engine",children:"Distributed Data Engine"})]}),"\n",(0,i.jsx)(n.li,{children:"Explore prompt optimization patterns for better responses"}),"\n",(0,i.jsx)(n.li,{children:"Set up monitoring and analytics for your system performance"}),"\n",(0,i.jsxs)(n.li,{children:["Scale your system using Kamiwaza's ",(0,i.jsx)(n.a,{href:"../architecture/overview",children:"distributed architecture"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Your RAG pipeline is now ready to answer questions using your own documents! Experiment with different models, chunk sizes, and retrieval strategies to optimize for your specific use case."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);