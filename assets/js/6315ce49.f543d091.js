"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[1913],{1140:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/01/13/agentic-computing-is-here-just-not-evenly-distributed","metadata":{"permalink":"/kamiwaza-docs/blog/2025/01/13/agentic-computing-is-here-just-not-evenly-distributed","source":"@site/blog/2025-01-13-agentic-computing-is-here-just-not-evenly-distributed.md","title":"Agentic Computing is Here, Just Not Evenly Distributed","description":"The future of computing is more personal and much more powerful. We are at an interesting stage presently where the capabilities of LLMs and the end-user experience are dramatically disconnected; a huge amount of engineering is yet to be done.","date":"2025-01-13T00:00:00.000Z","tags":[],"readingTime":10.87,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Agentic Computing is Here, Just Not Evenly Distributed","description":"The future of computing is more personal and much more powerful. We are at an interesting stage presently where the capabilities of LLMs and the end-user experience are dramatically disconnected; a huge amount of engineering is yet to be done.","date":"2025-01-13T00:00:00.000Z","image":"/img/blog/images/2025-01-13-swarm-of-agents.png"},"unlisted":false,"nextItem":{"title":"Using Kamiwaza Authentication and Model Inferencing with Any AI Application","permalink":"/kamiwaza-docs/blog/2025/01/03/adding-kamiwaza-auth-and-models-to-your-app"}},"content":"The future of computing is more personal and much more powerful. We are at an interesting stage presently where the capabilities of LLMs and the end-user experience are dramatically disconnected; a huge amount of engineering is yet to be done.\\n\\nThis last week we were [presenting on Agentic AI at the Rocky Mountain AI Interest Group](https://www.linkedin.com/posts/dan-murray_we-had-a-blast-at-last-nights-meeting-of-activity-7283208890031226880-4-dn).\\n\\nWe had a blast showing off [Kamiwaza](https://kamiwaza.ai), and a bunch of cool agentic tools. One of those was [STORM](https://storm.genie.stanford.edu/), from Stanford. It was fairly trivial to integrate with Kamiwaza. This meant I was able to use a reasonably high-quality model both on a hosted server or locally on my MBP (which is admittedly beefy, but we will see [a lot more mega-powerful AI personal computing options soon](https://www.nvidia.com/en-us/project-digits/), and there were **audible gasps** when I showed what it was doing under the hood, which involved the tool combing through literally **hundreds** of links to satisfy my request. Behind the scenes, I could confirm it - the tool that was doing search lookups retrieved over 900 search pages, navigating and slurping up that data.\\n\\n![A browser pointed at localhost showing part of hundreds of links browser agentically](/img/blog/images/2025-01-13-storm-browser-links.png)\\n\\nThe end result of this process is a conversation among agentic experts:\\n\\n![The STORM tool, showing conversations with various experts](/img/blog/images/2025-01-13-storm-experts.png)\\n\\nThe STORM tool uses persona-based processes. In this case, my simple query about cheeses which are similar to Manchego (a personal favorite), spawns a collection:\\n\\n- Cultural Anthropologist\\n- Historian of Cheese  \\n- Basic Fact Writer\\n- Cheesemaker from La Mancha\\n- Cheese Sommelier\\n- Nutritionist\\n\\nThe AI agentically creates an interview/response style setup with each of these personas, integrating this huge body of retrieved content, in order to produce a final output.\\n\\n![A Final Article (on cheeses similar to manchego) produced by STORM](/img/blog/images/2025-01-13-storm-final-article.png)\\n\\nAs a tech demo, this is super fun. I took a random topic from the audience and we watched it research and output as we talked and reviewed other demos. This is something extremely different from the \\"typical experience\\" of AI right now. This is no casual tool - using a model on a server capable of generating 1000+ tokens/sec, with the ability to comb through 900+ results on 90+ pages of searches, read and process hundreds of pages, and then generate hundreds of thousands of tokens of AI-powered discussion, you get a completely different outcome.\\n\\nEven as a daily user of [Perplexity](https://perplexity.ai), this is a \\"We Are Not the Same\\". I want to joke, \\"You want to see a real **Pro Search**? Let me show you what that really looks like.\\"  Because as good as Perplexity is, it does not come anywhere near what I was able to run myself. By looking through >100x as many links, reading far more material, having a for more powerful (6 agent personas and an agentic interviewer!) process, I can a massively better outcome.\\n\\n## What does this mean for the future of computing?\\n\\nChatGPT is an awesome tool. I remember being moved to [wish it a happy birthday](https://www.linkedin.com/posts/matthewwallaceco_chatgpt-reinvent2023-activity-7136034846186893312-2to-?utm_source=share&utm_medium=member_desktop), and I don\'t think I\'ve ever wanted to celebrate the birthday of a tool/service before in my life -- but ChatGPT gave people a **bad impression about what AI could do**. Because zero-shot queries to a model do not represent anything like the best; but they set the tone for what AI could, would, and should do. We are slowly moving away from that. But the world hasn\'t really grokked how powerful this can be - the personal acquisition, dissemination, and contextualization of knowledge in pursuit of our mental goals.\\n\\nWe got used to the idea long ago that the Internet was far too vast for us. We \\"surfed\\" our way through various links, following a web of interconnection. The underlying technology is still there, but one of the biggest trends of this for 30 years has been that of the **aggregator**. The first great aggregator in my mind was Yahoo. Their collections of links was of enormous value to users - and the ability to get into the Yahoo links equally powerful. There was a brief time when people when \\"AAA Widgets and Things\\" or \\"!! Double Bang Widgets and Things\\" was the sort of naming convention of choice to be on the top of those pages.\\n\\nWe had a host that followed - Google and Google News, Facebook, Twitter, Instagram, Pinterest. Of course beyond just links and images, there was aggregation of discussion too - Slashdot, Reddit, Quora and so on. Different takes on a theme - but the theme was to collect the content of the Internet and the contributions of users, repackage it for eyeballs, and sell the eyeballs to advertisers.\\n\\nFundamentally, this era is over. We won\'t see the decline for a while. But these things worked because they served the user. If you wanted News, Google News was great because you could peruse 10 headlines on a topic in a moment, grok the theme, drill down where you desired to. Facebook let you get a feed of things that people you knew were up to, and what they wanted to share. Twitter did the same with more anonymity and a less crowded format. Reddit let people form communities of interest and applied the wisdom of the crowd to the rankings.\\n\\nWhat does it mean when you can proactively publish, sort, and retrieve information? \\n\\nFurther, what does it mean when **even the quality of sources is agentically rated**? So you can algorithmically tailor things based on whatever your preferences are? It could be your version of trusted sources; or trusted sources of those you trust; it could be tone, topic. It could be based on assessments of academic rigor. And the feedback mechanisms could be as sticky as you wanted. Think source &lt;X&gt; is really frustrating? Block it, and your agent not only keeps it out of your feed, it keeps all the derivative material citing it out too.\\n\\nMeanwhile, your agents can network with other agents, building a web of trust to promote and expose higher-quality content.\\n\\nThe old promotion of content was aggregation as a means of reaching users. Agentic AI potentially gives us the chance to, as a whole, promote content not because it belongs to the ideal \\"syndicate\\" but because it is - well, the best content.\\n\\nIn some ways, I blame the status quo on Google. They killed [Google Reader](https://www.theverge.com/23778253/google-reader-death-2013-rss-social), and Reader was a dominant RSS tool at the time. Bloglines was another, and it was having major issues and was overtaken by Reader. \\n\\nIn a sense it\'s easy to potentially see why: Reader and similar tools lacked the social context. They were also noisy - because you were often \\"reading\\" RSS feeds which were sometimes hundreds of articles; perhaps some form of aggregation behind. A great example for me personally was the presence of [BoingBoing](https://boingboing.net/) on my feed. I felt like BoingBoing had a lot of great content, but the volume was pretty high and the content was pretty eclectic, so I was prone to \\"mark all read\\" without really processing it.\\n\\nSo we ultimately end up, even today, dealing with really crappy experiences just in terms of consuming content. We can:\\n\\n- Select narrowly focused sources, peruse them manually or in batch, but they\'re disconnected largely from social experience\\n- Leverage social feeds, and suffer painfully bad signal:noise ratios\\n\\nAI is going to put an end to this, by merging the social and the topical.\\n\\nHow? It\'s going to take niche sources (say, my interest in the excellent [Semianalysis](https://semianalysis.com/2024/12/03/amazons-ai-self-sufficiency-trainium2-architecture-networking/)) and curate them. It won\'t just be a feed - it will be a feed that is further cut down by my interests. AI can pre-read, and it can:\\n\\n- **Curate**: removing things that are from an interesting source but not an interesting (to me) topic\\n- **Enrich**: for things off the beaten path, it can anticipate my further interest and collect more data from dozens more sources\\n- **Summarize**: for things that are interesting but perhaps only in the macro, summarize for me\\n- **Contextualize**: it can remind me of other things. The longer I dwelled on something, or the more I like to share it/discuss it, the more it will want to contextualize around those topic clusters.\\n\\nAnd more! But critically the **future of AI and the Internet is much more proactive**. If it follows that for many time is our most precious resource, the idea that **AI will front-run our activities by a large margin** is \\"obvious\\". We will swim through a sea of meeting notes, project plans, idea boards. The mission of AI in this will be to massively amplify our agency. Pull more in, filter it more tightly, leverage it more powerfully.\\n\\n## Where is this vision?\\n\\nEmerging rapidly. I mentioned STORM already with my own integration, but, there are similar tools.\\n\\n[NotebookLM](https://notebooklm.google/) is one of those tools sitting at the intersection of fun+useful. Take it a step further and imagine Notebook LM as selectively social, selectively search-oriented.\\n\\nYou see hints of this in Perplexity Spaces.\\n\\nYou see hardware vendors creating more tools that allow AI to be personal, unmetered, private - whether it\'s Apple and their Apple Intelligence (or simply the ability on a MBP to run fairly large models and the [community that produces quantizations](https://huggingface.co/mlx-community), whether for mlx or [llamacpp](https://github.com/ggerganov/llama.cpp)), the [nVidia DIGITS hardware](https://www.nvidia.com/en-us/project-digits/), etc. The \\"arrows are pointed in the right direction\\" - models are getting smarter and cheaper, hardware is getting cheaper. The overall cost of \\"intelligence\\" has [declined massively](https://www.linkedin.com/posts/appenz_the-cost-of-an-llm-with-the-quality-of-the-activity-7270873006233530368-qfDA). \\n\\nThere\'s still enormous amounts of engineering to be done here, and none of the hardware, software and models is where they probably ought to be for anything like a final form factor. That said, the ability to easily get a massive level-up on the experience of research combining open source + AI at the cost of pennies shows that we are still heading toward massive disruption of the status quo.\\n\\nDepending on how the build-out goes, we may even see the return of some of the memes of web3 - the idea of a \\"new Internet\\" with less aggregation and more control to the end users could truly happen.\\n\\n## The Long Tail of Possibility\\n\\nThere are a lot of strange side-effects that could emerge from this. If your agents knew your preferences, and your opinions about existing products, could your agents earn you money by participating on your behalf in user research groups so people could build better products? Could those cost less because your agent would bring you into the fold of awareness because it knew the product solved the problem better for you, saving that new business on advertising and customer acquisition?\\n\\nIt\'s interesting to consider how we might end up spending more on infrastructure (because search+agents will cost an order of magnitude more than just search) but have that spend be more virtuous (because it would not be products competing for your attention, they would be competing for your approval/endorsement - if a network of virally connected ai agents able to personalize everything can curry good news to you).\\n\\nIn terms of education, awareness, community - these things could all see a similar revolution. On the [Kamiwaza.AI](http://Kamiwaza.AI) slack, we get notifications about interesting GitHub repositories that are trending that fit our interests:\\n\\n![Slack notification showing trending GitHub repositories](/img/blog/images/2025-01-13-slack-trending.png)\\n\\nWhen we continuously enrich, personalize, and contextualize everything we spend our time and energy on in this way, how much better are the outcomes?\\n\\n## The Shameless Plug\\n\\nSome of these concepts tie into why we are building [Kamiwaza.AI](http://Kamiwaza.AI) - as a platform, we offer a way to normalize a lot of the developer experience of integrating software into a model inference ecosystem. Moreso, we are doing so in a data-conscious way, with a stack with the ability to understand what data lives where, who should be allowed to access it, with the ability to localize that access. When we recently wrote about [integrating Kamiwaza.AI authentication and models](https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2025/01/03/adding-kamiwaza-auth-and-models-to-your-app) into other tools with only a handful of lines of code, it was to show how easy it was to bring new tools \\"into the fold\\". While we are oriented around Enterprise use, we offer a [community edition](https://github.com/kamiwaza-ai/kamiwaza-community-edition/) that is great for single systems or consumer devices (e.g., I run it constantly on my Macbook Pro - albeit the specs are a bit \'hungry\' right now but we are slimming it down) - and it then allows the software to become more personal and portable. Model selection works across models, hardware platforms, inference engines - the exact same code would work in the same way. The authentication integration to Kamiwaza works across local, OAuth, or SAML.\\n\\nIn the future, we will show the same applied to data, so you can more easily see how to integrate private or semi-private enterprise data sources, but in a portable fashion.\\n\\nBut this isn\'t pure advocacy - because we remain keenly interested in seeing this vision realized for the world. The product is a means to an end, which is a smarter, more agile, more interconnected world."},{"id":"/2025/01/03/adding-kamiwaza-auth-and-models-to-your-app","metadata":{"permalink":"/kamiwaza-docs/blog/2025/01/03/adding-kamiwaza-auth-and-models-to-your-app","source":"@site/blog/2025-01-03-adding-kamiwaza-auth-and-models-to-your-app.md","title":"Using Kamiwaza Authentication and Model Inferencing with Any AI Application","description":"How to integrate Kamiwaza\'s authentication and model inferencing into any AI application.","date":"2025-01-03T00:00:00.000Z","tags":[],"readingTime":4.635,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Kamiwaza Authentication and Model Inferencing with Any AI Application","description":"How to integrate Kamiwaza\'s authentication and model inferencing into any AI application.","date":"2025-01-03T00:00:00.000Z","image":"/img/blog/images/2025-01-03-kamiwaza-private-gpt.png"},"unlisted":false,"prevItem":{"title":"Agentic Computing is Here, Just Not Evenly Distributed","permalink":"/kamiwaza-docs/blog/2025/01/13/agentic-computing-is-here-just-not-evenly-distributed"},"nextItem":{"title":"2024: A Year in Review for GenAI","permalink":"/kamiwaza-docs/blog/2024/12/31/a-year-in-review"}},"content":"The Kamiwaza Enterprise AI Platform provides a comprehensive suite of tools for building and deploying AI applications in enterprise environments. This guide focuses on two specific features of the platform: enterprise authentication and model inferencing. We\'ll demonstrate how to integrate these features into your application using our port of the Vercel AI Chatbot as an example. All of the code in this guide is available in the [Kamiwaza Vercel AI Chatbot repository](https://github.com/kamiwaza-ai/ai-chatbot).\\n\\n## Enterprise Authentication Integration\\n\\nKamiwaza\'s authentication system provides centralized auth management that can be integrated into any application. Rather than building your own auth system, you can leverage Kamiwaza\'s secure token-based authentication and user management. The platform also supports advanced authentication features including role-based access control, group management, and organization-level permissioning - enabling enterprise-grade user management beyond basic authentication. While this guide focuses on core authentication, these additional features can be leveraged through the same centralized auth service.\\n\\n### How It Works\\n\\n![Authentication Flow Diagram](/img/blog/images/2025-01-03-auth-flow-diagram.svg)\\n\\nWhen integrating with Kamiwaza auth:\\n1. Your application sends login requests to Kamiwaza\'s auth service\\n2. Kamiwaza validates credentials and returns a JWT token in a secure HTTP-only cookie\\n3. Your application verifies the token\'s validity with Kamiwaza for protected routes\\n\\nNow, your application can use the token to access protected routes and resources.\\n\\n### Setting Up the Auth API Interface\\n\\n```typescript\\n// lib/kamiwazaApi.ts\\nconst KAMIWAZA_API_URI = process.env.KAMIWAZA_API_URI;\\n\\nexport async function login(username: string, password: string): Promise<LoginResponse> {\\n  const response = await fetch(`${KAMIWAZA_API_URI}/api/auth/token`, {\\n    method: \'POST\',\\n    headers: {\\n      \'Content-Type\': \'application/x-www-form-urlencoded\',\\n    },\\n    body: new URLSearchParams({\\n      username,\\n      password,\\n    }),\\n  });\\n\\n  if (!response.ok) {\\n    throw new Error(\'Login failed\');\\n  }\\n\\n  return response.json();\\n}\\n\\nexport async function verifyToken(token?: string): Promise<UserData | null> {\\n  const response = await fetch(`${KAMIWAZA_API_URI}/api/auth/verify-token`, {\\n    headers: { \'Cookie\': `access_token=${token}` }\\n  });\\n\\n  if (!response.ok) {\\n    return null;\\n  }\\n\\n  return response.json();\\n}\\n```\\n\\n### Managing Authentication State\\n\\n```typescript\\n// lib/auth-context.tsx\\nexport function AuthProvider({ children }: { children: ReactNode }) {\\n  const [user, setUser] = useState<UserData | null>(null);\\n  \\n  useEffect(() => {\\n    const checkAuth = async () => {\\n      try {\\n        const userData = await verifyToken();\\n        if (userData) {\\n          setUser(userData);\\n        }\\n      } catch (error) {\\n        setUser(null);\\n      }\\n    }\\n    checkAuth();\\n  }, []);\\n\\n  return (\\n    <AuthContext.Provider value={{ user }}>\\n      {children}\\n    </AuthContext.Provider>\\n  );\\n}\\n```\\n\\nThis integration provides your application with:\\n- Secure token-based authentication\\n- Centralized user management\\n- Session handling via HTTP-only cookies\\n- Built-in token verification and refresh\\n\\nThe AuthProvider component wraps your application and manages the authentication state, while the Kamiwaza service handles all the security-critical operations like token generation and validation.\\n\\n## Model Integration\\n\\nModels deployed through the Kamiwaza Platform can be easily integrated into any AI application. While models are deployed and managed through Kamiwaza\'s dashboard, accessing them in your application is straightforward - you just need to point your application to the deployed model\'s endpoint. This enables you to use Kamiwaza-hosted models alongside other LLM providers like OpenAI or Anthropic, giving you flexibility in model selection while maintaining a consistent integration pattern.\\n\\n![Model Inference Diagram](/img/blog/images/2025-01-03-model-selection-diagram.svg)\\n\\n### How It Works\\n\\n- Models are deployed and managed through the Kamiwaza dashboard\\n- Each deployed model is accessible via a specific endpoint and port\\n- Your application can fetch available models using Kamiwaza\'s deployment API\\n- Once selected, models can be used through a familiar OpenAI-compatible interface\\n\\nThis approach allows you to switch between different models without changing application code, use multiple models in the same application, and maintain a consistent API interface regardless of the model provider.\\n\\n### Integrating Models in Practice\\n\\n```typescript\\n// app/kamiwaza/actions.ts\\nexport async function getKamiwazaDeployments() {\\n  const response = await fetch(`${KAMIWAZA_API_URI}/api/serving/deployments`);\\n  if (!response.ok) {\\n    throw new Error(\'Failed to fetch deployments\');\\n  }\\n  \\n  const data = await response.json();\\n  return data.filter((d: Deployment) => d.status === \'DEPLOYED\')\\n    .map((d: Deployment) => ({\\n      ...d,\\n      instances: d.instances.map(instance => ({\\n        ...instance,\\n        host_name: instance.host_name || \'localhost\'\\n      }))\\n    }));\\n}\\n\\n// components/model-selector.tsx\\nexport function ModelSelector({ onModelSelect }: ModelSelectorProps) {\\n  const [deployments, setDeployments] = useState<Deployment[]>([]);\\n\\n  useEffect(() => {\\n    const fetchDeployments = async () => {\\n      const fetchedDeployments = await getKamiwazaDeployments();\\n      if (fetchedDeployments.length > 0) {\\n        const deployment = fetchedDeployments[0];\\n        const modelInfo = {\\n          baseUrl: `http://${deployment.instances[0].host_name}:${deployment.lb_port}/v1`,\\n          modelName: deployment.m_name\\n        };\\n        onModelSelect(modelInfo);\\n      }\\n    };\\n\\n    fetchDeployments();\\n  }, []);\\n}\\n```\\n\\n### Using Models for Inference\\n\\n```typescript\\n// lib/chat/actions.tsx\\nconst modelClient = createOpenAI({\\n  baseURL: getDockerizedUrl(selectedModel.baseUrl),\\n  apiKey: \'kamiwaza_model\'\\n});\\n\\nconst result = await streamUI({\\n  model: modelClient(selectedModel.modelName),\\n  messages: messages,\\n  text: ({ content, done }) => {\\n    // Handle streaming response\\n  }\\n});\\n```\\n\\n\\n## Example Implementation: Vercel AI Chatbot\\n\\nWe\'ve provided a complete example implementation that demonstrates these concepts in action. You can deploy your own instance of our Kamiwaza-enabled chatbot using Docker:\\n\\n1. Clone the repository:\\n```bash\\ngit clone https://github.com/kamiwaza-ai/ai-chatbot\\ncd ai-chatbot\\n```\\n\\n2. Configure environment variables:\\n```bash\\ncp .env.example .env\\n```\\n\\nUpdate the `.env` file with your configuration:\\n```bash\\n# Kamiwaza Configuration\\nKAMIWAZA_API_URI=http://localhost:7777\\nNEXT_PUBLIC_KAMIWAZA_API_URI=http://localhost:7777\\n\\n# Model Configuration (optional - for fixed model setup)\\nFIXED_MODEL_URI=http://localhost:8000/v1\\nFIXED_MODEL_NAME=\'Dracarys2-72B-Instruct-4bit\'\\nALLOW_ANONYMOUS=false\\n\\n# Authentication (generate using: openssl rand -base64 32)\\nAUTH_SECRET=your-generated-secret\\nNEXTAUTH_SECRET=your-generated-secret\\nNEXTAUTH_URL=http://localhost:3003\\n\\n# Redis Configuration\\nKV_URL=redis://localhost:6379\\nKV_REST_API_URL=None\\nKV_REST_API_TOKEN=dummy_token\\nKV_REST_API_READ_ONLY_TOKEN=dummy_readonly_token\\n\\n# Required but can be set to noop for Kamiwaza\\nOPENAI_API_KEY=noop\\n```\\n\\n3. Build and start the application using Docker:\\n```bash\\ndocker-compose build\\ndocker-compose up -d\\n```\\n\\nThe application will be available at `http://localhost:3003`. You can now log in using your Kamiwaza credentials and start chatting with your deployed models. The Docker Compose configuration will automatically set up both the chatbot application and its required Redis database for storing chat history.\\n\\nThe complete example code demonstrates authentication integration, model selection, and chat functionality in a production-ready application context. If you\'re interested in learning more about how Kamiwaza can help you build your own AI applications, please [contact us](https://kamiwaza.ai/contact) for a demo.\\n\\n\\n&nbsp;"},{"id":"/2024/12/31/a-year-in-review","metadata":{"permalink":"/kamiwaza-docs/blog/2024/12/31/a-year-in-review","source":"@site/blog/2024-12-31-a-year-in-review.md","title":"2024: A Year in Review for GenAI","description":"A look back at the year in GenAI","date":"2024-12-31T00:00:00.000Z","tags":[],"readingTime":16.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"2024: A Year in Review for GenAI","description":"A look back at the year in GenAI","date":"2024-12-31T00:00:00.000Z","image":"/img/blog/images/2024-12-31-year-in-review.png"},"unlisted":false,"prevItem":{"title":"Using Kamiwaza Authentication and Model Inferencing with Any AI Application","permalink":"/kamiwaza-docs/blog/2025/01/03/adding-kamiwaza-auth-and-models-to-your-app"},"nextItem":{"title":"Harnessing the Power of Large Language Models","permalink":"/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware"}},"content":"On the last day of 2024, it is a good time to reflect on the changes. What a year it has been.\\n\\n## Big advancements\\n\\nIt\'s wild to consider that Claude 3 Opus was only released in March of 2024. It was super expensive, slow, and not decisively better than ChatGPT in any meaningful way, but it was better. Of course, the fast-follow of Sonnet 3.5 was huge for Anthropic - they finally landed something in the sweet spot of quality and cost, with its specialty expertise in code letting it make serious headway.  That was Jun 20, 2024.\\n\\nIt was also the **Year of the Llama** as Meta\'s Llama 3 launched Apr 18, 2024. It was followed with 3.1, 3.2, and 3.3, and supporting infrastructure of [Llama Stack](https://github.com/meta-llama/llama-stack) which is probably underappreciated at the moment.\\n\\nThen there was the endless drama about OpenAI\'s [model code-named Strawberry](https://www.oneusefulthing.org/p/something-new-on-openais-strawberry) which turned out to be the o1 family. It\'s only really just not getting released to the public - OpenAI has a real knack of having things that look amazing and taking a really long time to release them. Some, like **o1**, turn out to be pretty solid. Others (looking at you, **Sora**) just end up being disappointing.\\n\\n## Open Models\\n\\nIt was just prior to NeurIPS 2023 in December 2023 when Mistral famously dropped their quiet torrent of the 8x7B model. In many ways, I felt like that was a bellweather moment for open source AI. This was the \\"as good as 3.5 Turbo at home!\\" moment.\\n\\nFor context, neither of those models ranks **in the top 100** of the LMSys Arena Leaderboard.\\n\\nInstead, the accidental Llama 2 followed by Mistral\'s models, Llama 3, and then open source models like Qwen 2.5, Deepseek v2/2.5/3 led us to this auspicious moment.\\n\\n![Code Leaderboard](/img/blog/images/2024-12-31-code-leaderboard.png)\\n\\nThis isn\'t just theory - if you dug around in the [Kamiwaza](https://kamiwaza.ai) Slack, you\'d find several occasions where our developers would report Qwen2.5-Coder-32B solving something where both Claude 3.5 Sonnet and GPT-4o would fail.\\n\\n## OpenAI and Anthropic retain a lead, and Google may have finally arrived\\n\\nOn the other hand, I\'d be remiss if I didn\'t mention that OpenAI and Anthropic held onto a lead. Anthropic\'s Claude 3.5 Sonnet is simply amazing at code and in general, for what used to be the most obsequious of models it has a great tone and can be downright funny. OpenAI\'s GPT-4o remained excellent, but was never as good as Claude 3.5 Sonnet in coding tasks. However, OpenAI released o1, and the o1 models continue to be astoundingly good. They\'re simply slow. One can see a clear connection though.\\n\\nGoogle may have finally awakened the sleeping giant. Gemini 2.0 flash is a beast especially if it remains at \\"flash\\" pricing. I\'d say that:\\n\\n\x3c!-- truncate --\x3e\\n\\n* OpenAI\'s o1 clearly has a reasoning edge especially on high compute, with the unreleased o3 appearing from the teasing to be absurdly strong (and almost unusably expensive)\\n* Claude 3.5 Sonnet is still the best code model, with o1 pro/full possibly edging it out barely, but in most cases this being an academic victory due to Sonnet being much faster/cheaper\\n* Gemini models have serious potential - but Flash is oddly strong in some areas, oddly weak in others (coding, for example, where Gemini 2.0 flash underperforms both Qwen 2.5 and DeepSeek v3 along with Sonnet and o1)\\n\\n## Applied GenAI Engineering is still lacking\\n\\nEarliest adopters: tech startups, large SaaS, cloud providers.\\n\\nThose were your early GenAI leaders.\\n\\nStartups like:\\n\\n* Jasper\\n* Character AI\\n* Perplexity\\n* Midjourney\\n\\nSaaS companies like:\\n\\n* Notion\\n* Adobe\\n* Salesforce\\n* ServiceNow\\n\\nAnd of course the CSPs.\\n\\nWhat was really lacking was big GenAI Enterprise success stories. I chalk this up to a few factors:\\n\\n* Enterprises value safety over velocity\\n* SaaS/Cloud/Tech companies are all acclimated in different ways to hyper-velocity, both culturally and structurally\\n* Enterprises often have under-softwared business processes when you look at their non-SaaS efforts; \\"the world is run on Excel\\" isn\'t too far off (and if you grouped Excel and COBOL together it\'s probably a lot more!)\\n* Enterprises are custodians of a lot of PII/PHI and other sensitive data\\n* There was a fair amount of early handwaving about the [Samsung code getting into ChatGPT](https://humanfirewall.io/case-study-on-samsungs-chatgpt-incident/)\\n\\n## Shadow AI: the ghost of Shadow IT / Shadow cloud\\n\\nStarting in the very late 00s, going into ~2015-2016 (which I think of as the \\"years of capitulation\\" to cloud), there was a lot of \\"shadow IT\\". Everyone played on this - vendors doing cloud PS etc would harp on this to sell their transformation/migration services; orgs like VMware (where I was employed from \'10-\'12) would use it as a way to advocate upgrading VMware environments to \\"private cloud\\" (the cloudwashing of \\"private cloud\\" is the stuff of legend now).\\n\\n**Now we have shadow AI.** This is employees who are forbidden from using various tools and services, but who are using it anyhow. The BYOD era and multi-modal models makes this easy. You have so, so many ways:\\n\\n* Vanilla \\"just use it\\"\\n* BYOD office tethering on cell to hit LLM services\\n* VPNs\\n* Take a pic/screenshot/email and other similar versions of \\"get around the firewall\\"\\n\\nThat last one is in some ways the most hilarious and bad - some employees are redistributing sensitive data 2-3x just to get around restrictions meant to avoid it.\\n\\nBy the way, a hard bad on all AI is very likely to keep this up no matter how hard you try to advocate to stop it. Counter it with:\\n\\n* Make good tools available - at the very least, an internal stack - [Kamiwaza](https://kamiwaza.ai) can help here with the ability to easily operationalize your own models and services, and our out-of-the-box tools include things like a chatbot. \\n\\n![Kamiwaza Chatbot](/img/blog/images/2024-12-31-kz-ss.png)\\n\\n* Select providers that have compatible privacy/security policies and educate your employees on settings - good example being that OpenAI lets **anyone** opt out of sharing data to improve the model, but for personal accounts this has to be turned on by the user.\\n* Educate and don\'t just dictate. Help the employees understand the *\\"why\\"* of the policies - being transparent about your regulatory and contractual obligations can help them understand the burden and adhere.\\n\\n### The real value in Enterprise AI\\n\\nThe real value in Enterprise AI use is not anything fancy. It\'s actually not anything groundbreaking at all except in efficiency. It\'s the boring stuff - repetitive, low-value but utterly necessary tasks - where AI excels. Consider how many people have a signifiant part of their job being the non-challenging knowledge work:\\n\\n* Parsing documents\\n* Making computations\\n* Entering values into systems\\n* Getting values out of systems\\n* Running (or writing but less so) reports\\n\\nThese activities are often linked to huge stables of \\"line of business\\" applications - software where it is too specific to the enterprise to be pulled into a platform like Salesforce or ServiceNow, but which is nonetheless necessary. These are ripe for AI - not just to agentically handle the tasks, but to upgrade/improve that software.\\n\\nThis is so remarkably strong that one bright spot is the assault on vertical SaaS. There\'s a great [a16z blog on it](https://a16z.com/ai-wedges-will-help-startups-outmaneuver-incumbents/). As Joe Schmidt wrote, the startups are \\"drinking the milkshakes\\" - a reference to the amazing **There Will Be Blood** meme:\\n\\n![Drink Your Milkshake](/img/blog/images/2024-12-31-milkshake.png)\\n\\nAn incredible movie and an incredible strategy.\\n\\n**Rethink everything you know about the nature of software, because as the cost of LLM tokens goes down and model intelligence goes up, the cost of software is on its way to zero.**\\n\\nThis is so unbelievably profound. As a developer you can opt to be terrified by this (very reasonable) or utterly thrilled. I cannot predict what happens here. We are seeing the unmovable object meet the unstoppable force here -- on one hand, ~free tokens means AI-powered software engineering is going to replace and automate an unbeleivable amount of work. Some will be done automatically. Some will be done assisted. Some little left will be done entirely by human ingenuity. \\n\\nBut that\'s only one force. The world is filled with bad or missing software.\\n\\n### What happens when you can spend an afternoon with a tool and have a full clone of Nextdoor+Facebook for your neighborhood?\\n\\nNo one knows this answer, but this is a thing underlying [Jevon\'s Paradox](https://en.wikipedia.org/wiki/Jevons_paradox). As the cost of some things goes down, demand goes up. And there is a **very clear picture** that demand is much, much higher for software if the software is ultra-cheap.\\n\\nIt also disaggregations the entire \\"software-industrial complex\\". I mean the chain of designers, frontend devs, backend devs, agile, scrummasters, project and program management, software engineering managers - it\'s unbelievably complex and cumbersome. The fungibility of AI is so wild. At [kamiwaza](https://kamiwaza.ai) we\'re bringing an app garden out. It\'s a great fit, as tons of apps work really well, especially privately, if you just give them a decent model and make deployment easy. \\n\\nSo rather than trying to break out Balsamiq like I would 10 years ago, \\"*Build me a page that...*\\"\\n\\n![App Garden Mockup 1](/img/blog/images/2024-12-31-mockup1.png)\\n\\n![App Garden Mockup 2](/img/blog/images/2024-12-31-mockup2.png)\\n\\n![App Garden Mockup 3](/img/blog/images/2024-12-31-mockup3.png)\\n\\n(*As I typed this, AI even autocompleted most of the bottom 2 images*)\\n\\nThis era of \\"do more better cheaper faster\\" is here now, it\'s just not evenly distributed yet.\\n\\n## The Dawn of AGI\\n\\nI wrote a [lengthly post on OpenAI o3 conquering the ARC-AGI bnechmark](https://www.linkedin.com/posts/matthewwallaceco_the-arc-agi-is-not-a-typical-evaluation-activity-7276229988905885697-japT). People have differing opinions on how legitimate this is, or how significant. I\'m by no means the final authority on such a thing; but as I outline, I do believe that Fran\xe7ois Chollet has been a consistent skeptic of \\"thinking\\" by LLMs, and when he says\\n\\n> It scores 75.7% on the semi-private eval in low-compute mode (for $20 per task in compute ) and 87.5% in high-compute mode (thousands of $ per task). It\'s very expensive, but it\'s not just brute -- these capabilities are new territory and they demand serious scientific attention.\\"\\n\\nThis is an extremely notable departure from that skepticism. He\'s not calling it AGI. That definition is much-debated and stories like [OpenAI and Microsoft saying $100B in profit is what defines it](https://gizmodo.com/leaked-documents-show-openai-has-a-very-clear-definition-of-agi-2000543339) isn\'t helping.\\n\\nThat said, having started to get a good amount of stick time with **o1-pro**, I will say its answers are materially stronger than average. It sheds the **gpt-4o** tendency to make \\"easy\\" mistakes or, worse \\"forget\\" input context. (For what it\'s worth, Claude was an early \\"look at our huge context\\" leader but that context was absolute trash.)\\n\\nIt\'s an interesting contrast because a lot of people listened to [Ilya Sutskever\'s NeurIPS talk](https://old.reddit.com/r/singularity/comments/1hdrjvq/ilyas_full_talk_at_neurips_2024_pretraining_as_we/) and thought he said \\"pretraining is over\\" - which he did in a sense, \\"we have but one Internet\\" and called it \\"the fossil fuel of AI\\".\\n\\nBut those people leave out his next slide:\\n\\n![Ilya\'s second slide](/img/blog/images/2024-12-31-ilya-slide2.png)\\n\\nThese are all interesting. I am not a researcher so as an expert at reading the tea leaves I could prognosticate but that\'s a story for another post.\\n\\nSuffice it to say, the story of LLMs in the past 2 years is:\\n\\n* Clean data is better\\n* More clean data is better\\n* LLMs get better results when trained to approach the data in different ways (papers on training LLMs to do math well are illustrative here if you care enough to dig into this!)\\n\\nBut beyond that:\\n\\n### Agentic workloads will drive feedback mechanisms\\n\\nIf you are an advanced Claude user, you\'re probably using [Model Context Protocol](https://github.com/modelcontextprotocol/) plug-ins. (You should.) But you may also have noticed how Claude 3.5 Sonnet is really smart about how to use them, about how OS-level things work - it\'s just \\"clean\\". I have exactly zero insider knowledge here, but I can practically guarantee you there is a substantial farm of Claude bots with a ton of purview to run around and build and evaluate stuff which slowly digest their energy back into really clean training data. I\'d love to know if any of it ends up in a pre-training zone, or if this is all used in fine-tuning/DPO/RLHF/etc efforts; but regardless.\\n\\n### Which are a form of synthetic data\\n\\nGetting a model to interact with anything which can provide full win/lose level reward function judgments is a form of synthetic data. This is one reason why code models are just so tempting: almost all code can be evaluated in a deterministic way, and often in a functional way as well. This provides the feedback mechanism to generate synthetic data to improve training. This is undoubtedly a major hope for OpenAI - that o1/o3 doing inference-time compute can generate mountains of synthetic data to improve training on non-thinking models. Pick any class of question (*\\"How many R\'s are in strawberry?\\"*) and if o1/o3 can get through it the vast majority of the time but the core model fails, that\'s likely an improvement that makes it into said model.\\n\\n## We are distilling human knowledge\\n\\nThis is what distillation of knowledge is. I don\'t mean distillation at the model level, which is about using the guidance of a bigger model to get it\'s intelligence into a smaller model - whether that\'s via tuning, training, or direct manipulation of the model\'s weights.\\n\\nNo, I mean \\"we are separating the wheat from the chaff\\". We are taking high quality data and using industrialized thinking to analyze it to continue to condense it to be the best version of itself. LLMs may remain stochastic, but a random selection between 10 excellent options is better than 1 excellent, 4 good, and 5 average-to-mediocre response options. This runtime-inference to synthetic training data pipeline is in its earliest days, so there\'s little to evaluate the efficacy **other than** the enormous success of high quality data in improving model performance in general.\\n\\nThe entire idea of [model collapse](https://www.nature.com/articles/s41586-024-07566-y) as one of those \\"much ado about nothing\\" things. (It\'s ironic that the Nature publication was ~6 months after the pre-pub, by which time this had gone from \\"*hey this is concerning*\\" to \\"*ok, actually, synethic model-generated data can be AWESOME for model quality when done right*\\")\\n\\n\\n## The Era of Tools is Upon Us\\n\\nIf you haven\'t yet been using Claude with MCP tools, you need to run-not-walk and give it a whirl.\\n\\n![MCP Tools](/img/blog/images/2024-12-31-claude-mcp.png)\\n\\nIt\'s simply a nice workflow. It\'s early days, but I feel relatively safe predicting:\\n\\n* There will be a proliferation of tools built around MCP, because developers have to write them and devs love Claude\\n* The architecture of the tools with both open clients and open servers strongly supports a network effect\\n\\nWhich means I think we are seeing a de facto standard for tools emerge.\\n\\nOpenAI may well be the last that wants to adopt this. Which makes sense, they had function calling so long ago - but to be fair, MCP looks like a master stroke compared to their implementation. Partially because of how open it is. Partially because of all the code. But most importantly:\\n\\n**It is baked into Claude the client in a very clean, super usable way and so adoption is going to be enormous.**\\n\\nIt\'s interesting that OpenAI made it so hard to leverage tools with GPT, which makes the Claude MCP version look like a master stroke. It\'s all local, it\'s all open. So something like the above where Claude is just editing code is super easy and takes less than 5 minutes to get going locally and the value is utterly off the chart. In many ways I find using ChatGPT a bit painful by comparison, not because of model quality, but because the tooling is just so much better.\\n\\nAnother example of tooling for the win here is the overwhelming popularity and love of Claude Artifacts. The artifact as an encapsulation of a piece of code or such is great, and the idea that Claude could render it to show you something like a preview of react is great too -- but the real win is the integration with projects. Getting the AI to take notes and adding it to the project is huge. It\'s very limited in a sense: all that data is just in the context for every request, which makes it eat a lot of token capacity, with the attendant implications for conversation length and number of messages you can effectively use. But it\'s so useful and Claude really does appear to treat all that material like it is just \\"in the conversation.\\n\\nThis is a remarkably different experience from a custom GPT on ChatGPT and giving it documents, where it clearly does some sort of RAG. Upside there is you can feed it a lot more data, but the downside is: the model is VERY likely to not notice it.\\n\\nEven in cases where you literally paste a lot of context into the chat window, GPT has a tendency to say something like:\\n\\n> Your file.py should probably look something like:\\n\\n```python\\n\\nif __name__ == \\"__main__\\":\\n    print(\\"Hello, world!\\")\\n```\\n\\nTo which you want to reply: **Why are you saying \\"should look something like\\", I literally pasted that file in with a \'# /path/to/main.py\'!!!!**\\n\\nWhat I\'m expecting to see is more open models trained EXPLICITLY on MCP. This will drive more adoption of MCP for tools, which will drive more model training.\\n\\nWhen ChatGPT gets MCP, explicitly or implicitly, that may feel like the final nail in the coffin. But Anthropic had massive pole position on this.\\n\\n\\n\\n## Hardware Remains an Unbelievable Blocker\\n\\n![nvda chart](/img/blog/images/2024-12-31-nvda.png)\\n\\nIt\'s been a great year for nVidia. Adding **$2T of EV** is the stuff of legend, but nVidia continues to enjoy a pretty amazing lead in both market share, mind share, and software sophistication that makes it the envy of the industry. It now has a lot of competition aiming to take some of its share (or, drink its milkshake, as it were).\\n\\nDespite that, they mentioned Blackwell being sold out a year in advance.\\n\\nThis is not any sort of surprise to me. There are all manner of GPU supply constraints. As I went to get custody of an 8xH100 SXM cluster, I was waiting in line. Azure is topping out at 2xH100 instances, you still can\'t even get an 8xA100 for a large model, etc. Constraints abound.\\n\\nThere was more drama behind Altman\'s supposed [\\"Fundraising for $7T\\"](https://www.calcalistech.com/ctechnews/article/222fqhh6m) which turned out to be him advocating for a much larger investment in chip design and fabrication. This makes sense, as nVidia\'s margin is, to some extent, OpenAI\'s opportunity and vice versa. (nVidia clearly owes a debt of gratitude to OpenAI for the AI boom, but that can only go so far. What have you done for me lately?)\\n\\nMore of the unstoppable force / immovable object analogy here:\\n\\n* nVidia is crushing it\\n* Blackwell is sold out\\n* Enterprise inference is **&lt;1%** of its final form\\n* Even while models are driving $/token costs through the ground\\n\\nMaybe some \\"real\\" analysis can shake this out - but there\'s clearly a tension here. And yet you can see that if o1/o3 are similar for quality models in the short term, we indeed must be lacking horsepower.\\n\\nClearly, models thinking through complex problems is useful but also inferentially costly.\\n\\n![qwq thinking](/img/blog/images/2024-12-31-qwq.png)\\n\\nAnd yet it\'s no surprise this works - eliciting chain of thought had a tendency to work better merely because of the autoregressive nature of the models - \\"changing your mind\\" isn\'t something a model does without tokens. And \\"just give your answer and no explanation\\" was actually a great way to tank results.\\n\\n\\n## In Conclusion\\n\\n2024 was a heck of a year. Outside of what\'s going on in the space, of course [kamiwaza](https://kamiwaza.ai) went from a dream to a product to a company, and now a well-funded one at that. Speaking of which, we\'re hiring!\\n\\nCheck out some details on our [open roles](/kamiwaza-docs/company/jobs)!\\n\\nHappy New Year!\\n\\n&nbsp;"},{"id":"/2024/04/15/a-primer-on-GenAI-hardware","metadata":{"permalink":"/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware","source":"@site/blog/2024-04-15-a-primer-on-GenAI-hardware.md","title":"Harnessing the Power of Large Language Models","description":"A primer on the hardware requirements for deploying and leveraging Large Language Models (LLMs).","date":"2024-04-15T00:00:00.000Z","tags":[],"readingTime":11.39,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Harnessing the Power of Large Language Models","description":"A primer on the hardware requirements for deploying and leveraging Large Language Models (LLMs).","date":"2024-04-15T00:00:00.000Z","image":"/img/blog/images/2024-04-15-llm-library.png"},"unlisted":false,"prevItem":{"title":"2024: A Year in Review for GenAI","permalink":"/kamiwaza-docs/blog/2024/12/31/a-year-in-review"}},"content":"Welcome to an illuminating journey through the world of hardware essentials for deploying and leveraging Large Language Models (LLMs). In this era of generative AI, the right hardware is not just an operational need; it\u2019s a strategic asset that can drastically elevate the performance, scalability, and efficiency of your AI applications. Whether you are integrating AI into your enterprise solutions or enhancing existing applications, understanding the interplay between hardware and AI capabilities is fundamental. Let\'s dive into the crucial hardware considerations that can help you kickstart and scale your AI initiatives effectively.\\n\\n## The Right Hardware for the Right Task: Starting Your AI Journey\\n\\nBefore diving into specifics, it\'s essential to understand why selecting the right hardware is critical for proving the value of AI in your business. The hardware you choose impacts everything from development speed to operational efficiency and scalability. It serves as the launching pad for your AI projects, setting the tone for innovation and performance outcomes.\\n\\n### Nvidia with Linux: The Leading Choice for AI\\n\\n#### Optimal for High-Performance AI Tasks\\n\\nNvidia GPUs, running on a Linux operating system, are a common sight in the world of AI hardware, largely due to Nvidia\'s significant market share and proven track record in AI applications. This combination is highly scalable, perfect for server setups where expansion is anticipated, and offers robust AI support with superior CUDA support for intensive computations.\\n\\n#### Considerations\\n\\n- **Driver Compatibility:** Linux requires careful management of GPU drivers and CUDA versions, which can be challenging in rapidly evolving setups.\\n\\n### Apple OSX with Metal: Seamless Integration for Mac Developers\\n\\n#### Best for Integrated Apple Ecosystems\\n\\nFor developers already embedded in the Apple ecosystem, OSX with Metal provides a streamlined performance pathway. Metal API is tailored to optimize AI operations specifically on Apple hardware, leveraging shared memory capabilities and accommodating large RAM sizes which are integral for complex AI tasks.\\n\\n#### Considerations\\n\\n- **Limited GPU Support:** Apple hardware typically does not support external GPUs as flexibly as PC setups, limiting performance scalability.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Windows Environments\\n\\n#### Versatile but with Nuanced Support\\n\\nWindows has long been a first class citizen for nVidia drivers due to its heritage in gaming. Indeed, nVidia has even release a [Chat With RTX](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/) local LLM/RAG app; the problem with Windows for Enterprise is that it can be much more challenging to see the direct path between development and production due to the large OS deltas. One reason Kamiwaza opted to build and support OSX hand-in-hand with Linux as a production environment was that the support on OSX was nearly like-for-like, providing a very consistent development -> production experience.\\n\\n## Understanding Model Scales: From SLM to LLM\\n\\nBefore discussing specific models, it\'s crucial to understand what a \\"model\\" in AI parlance means. Essentially, the term refers to the underlying AI that has been trained to understand and generate human-like text based on the data it has been fed. Models are measured in parameters\u2014a parameter being a piece of the model that has learned a specific part of the data during training. The scale of the model, from Small Language Models (SLMs) like Google\'s Gemma-2B, to an optimized open source 7B model like [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) for drafting emails, to Large Language Models (LLMs) like HuggingFace\u2019s Zephyr-Orpo 141B, a fine-tuning of Mistral Mixtral-8x22B models.\\n\\n### Small Language Models (SLM) & Developer Systems\\n\\n**Hardware Needs:** Can run efficiently on consumer-grade laptops or desktops typically; and even, depending on the hardware, directly on consumer mobile devices. Qualcomm announced in early 2023 a working version of Stable Diffusion [on an Android phone](https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android). That said, for prototyping we recommend either Macbook M* with ample RAM (the M2MAX/M3 Macbook Pros, or equivalent Mac desktops, are extremely popular with developers and other professionals, but with heavy RAM, 96-128GB, they can run extremely large models).\\n\\n### Large Language Models (LLM)\\n\\nFor larger models, heavy production use, you typically now want to turn to production-grade hardware. While the typical choice today is Linux servers with nVidia GPUs, there are a lot of alternatives here, including:\\n\\n- **CPU-based inference** on Intel AVX512-capable chips\\n- **Qualcomm** [AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100) series, with their AI 100 Pro and AI 100 Ultra, which are extremely powerful hardware with impressively low power consumption\\n- **AMD** [AMD Instinct](https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html) is AMD\'s answer to nVidia\'s dominance and sport impressively large VRAM and solid performance; but many caution around software support in newer libraries\\n- **nVidia** nVidia needs no introduction; their A100 and H100, with their upcoming GH200 and the recently announced [Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing) GPUs starting with the B100. nVidia has reaped the boom of GenAI very notably with orgs like Meta announcing a buy of [350,000 H100s](https://www.pcmag.com/news/zuckerbergs-meta-is-spending-billions-to-buy-350000-nvidia-h100-gpus), adding to an extremely large farm of GPUs already operated - this helps them attract and retain talent developing their in-house open-sourced LLMs, like [Llama 2](https://llama.meta.com/llama2/), arguably the most influential open-source AI model ever released.\\n\\n## Decoding Performance Metrics: Token Speed and VRAM\\n\\nWhen selecting hardware for AI, understanding performance metrics is crucial. Token speed and VRAM stand out as primary indicators.\\n\\n**Tokens/second** is just a metric of how fast a given model engine can generate output tokens. For a visual in-context view on model deployment and stress test, check out the [Kamiwaza Model Deployment and Inference Stress Test](https://www.youtube.com/watch?v=h4yyqfw9liY) video on YouTube.\\n\\n![Tokens Per Second CLI readout](images/blog_tokens_sec.png)\\n\\nIt\'s important to note that due to the way the hardware and software interact, there\'s a difference on many hardware platforms between the speed you would see in a single client generation (e.g., one prompt -> one response) vs batched. For example, in the image above, our stress test averaged almost 1200 tokens/second but the same card responding to a single request would likely only get ~40-60 tokens/second at best; but it can perform many parallel inferences when batching. Kamiwaza helps with model deployment to make this easier to manage.\\n\\n- **Model Size Impact:** Larger models, while capable of generating more complex texts, tend to operate more slowly due to their size. More **parameters** means more math operations for each pass through a model. You can think of it as a set of multiplication operations through dense matrices, each full pass popping out the next generated token based on the input context and probabilities in the neural network weights.\\n- **VRAM Requirements** As a rule, you can think of memory requirements being driven by three things:\\n\\n1. **The number of Parameters**: For a \\"70B\\" model, 70 billion parameters means 70 billion numbers in the neural network, effectively. Model models at \\"full weight\\" for inference at 16-bit, and you may see the term *float16* or *bfloat16* which are specific 2-byte data types. So a full-weight 70B model needs 140GB of memory to hold the weights. This number can be reduced through using a lower-precision model, through a technique called **quantization** which is essentially a neural-network specific version of \\"rounding\\" the numbers; so instead of a 16-bit number, each weight becomes an 8-bit or a 4-bit weight. (And quantization has more variety than that, such as quantizing different layers to different weights, as some are more performance-sensitive to that rounding)\\n2. **The context**: The user input must be converted into numbers also, called an embedding, to be processed by the network; there is the direct usage of the embedding, but there is also intermediate calculations and caches; the usage of these varies by architecture.  There are some tools like this [VRAM calculator on HuggingFace](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) that can help estimate; something like 1-2GB per 4096 input context tokens at a batch size of 512 isn\'t a terrible rule of thumb, but this varies quite a bit.\\n3. **Added KV Cache for Performance**: Engines like vLLM (the default deployment engine Kamiwaza deploys on when using Linux) pre-allocate additional memory for KV cache; this can dramatically speed up performance. vLLM will happily pre-allocate an entire 80GB card even on a 7B parameter model (so, one you could definitely run easily at full weight even on a 24GB consumer card), but it doesn\'t need that much. Kamiwaza\'s default deployment configs can be modified but try to allocate a recommended amount of memory by default for a performance boost, without being excessively generous.\\n\\n## Advancements in Model Efficiency\\n\\nIn the dynamic landscape of AI, continuous improvements are crucial for enhancing model performance and managing resource utilization effectively. Key techniques have emerged that not only boost computational efficiency but also optimize the overall functioning of AI models:\\n\\n- **Quantization:** This technique transforms the model to operate on lower precision (e.g., 8-bit integers instead of 16-bit floats), which can significantly increase the speed of computations and reduce the overall model size. This is particularly beneficial in environments with limited hardware capabilities or where rapid response times are crucial.\\n- **Mixture of Depths:** Adapting the depth of the neural network layers according to the task complexity can optimize processing time and power consumption. This approach tailors the resource allocation based on the immediate needs of the application, ensuring efficient use of computational power.\\n- **Dynamic Pruning:** By temporarily reducing the size of the neural network during computations, dynamic pruning helps conserve resources without a notable compromise in performance. This method is particularly useful in runtime environments where flexibility in resource allocation can lead to cost efficiencies.\\n\\nThese techniques represent the forefront of making large-scale models more accessible and manageable, paving the way for broader adoption across various industries.\\n\\n## Comprehensive Approach to AI Application Stack\\n\\nImplementing LLMs effectively requires a comprehensive understanding and integration of multiple components of the AI application stack. This holistic approach ensures that each layer is optimized for maximum performance and efficiency:\\n\\n1. **Hardware:** The foundational layer, which includes high-performance GPUs and CPUs, tailored to the needs of demanding AI tasks.\\n2. **Operating System:** Choosing the right OS\u2014whether it be Linux, OSX, or Windows\u2014is crucial as it must synergize with the hardware to optimize performance and provide stability.\\n3. **Model Engine:** Utilizing advanced frameworks like TensorFlow or PyTorch, which are designed to leverage the underlying hardware capabilities to the fullest.\\n4. **Application-Level Packages:** Tools such as LangChain and dSPY provide specialized functionalities that are essential for developing sophisticated AI applications. They serve as the building blocks for creating user-centric solutions that harness the power of LLMs.\\n\\nWith a solid grasp of these components, organizations can navigate the complexities of AI deployment more confidently, ensuring that their AI initiatives are both scalable and robust.\\n\\n## Bottom Line Recommendations\\n\\nThese are \\"inference only\\" recommendations; for training, there may be other considerations.\\n\\n### nVidia\\n\\n- At least 3 hosts for redundancy for Enterprise\\n- Homogenous hardware config for cpu/gpu/memory; Kamiwaza can deploy multiple models on larger cards\\n- **RTX4090** is often at performance parity with the datacenter A100 card for inference; they have 24GB of memory per card\\n-- You can find deployment options fairly standard for configs of up to 6x4090, giving 192GB of VRAM\\n-- Being a consumer card, the RTX4090 can typically be found at ~$2k/card, meaning the 6x setup is favorable vs a single A100 datacenter card\\n- **A100, H100** are the kings of the \\"available\\" datacenter-class hardware\\n-- You can build single hosts at up to 8x\\n-- They come in PCEe and SXM form factors\\n-- Both very widely available in cloud instances\\n-- 40GB and 80GB VRAM versions\\n-- Relatively expensive hardware (as of this writing, a single A100 80GB is still about $17,000, just for 1 card); although prices have been coming down lately\\n-- For the most powerful open models, 4x or more can be required. For example, the recently released Apache-licensed [Mixtral-8x22B-v0.1 model](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1) requires **~260GB** of VRAM to load the full-weight (non-quanitzied) version, plus memory for kv cache, and input context; this means this model can largely consume a **4xA100-80B** setup on its own. Commensurately, however, fine-tunes such as the [Microsoft WizardLM fine-tune](https://huggingface.co/microsoft/WizardLM-2-8x22B) are stronger than the original GPT4 model, which combined with its 64K context length makes it comparable to being able to operate a completely private GPT4-class model\\n-- In the middle ground, many organizations will find that base or fine-tuned version of models like [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) or [DeepSeek-Coder-33B](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct) will be appropriate; possibly at any weights from the full weight (using a single A100) to a strong 4-5 bit quantization (which can run on a single RTX4090, or locally on a well-equippted mac/macbook pro).\\n\\n### Other nVidia Hardware\\n\\nAs a footnote, there are a number of other cards that share similar architectures; for example, the nVidia Ada architecture powers the RTX4xxx series, but also the \\"pro\\" cards, like the RTX6000, as well the L40, sport (up to) 48GB of memory. Those can be reasonable choices as well.\\n\\nFor the curious, the highest-end cards, like the A100 and H100, excel at 32-bit workloads, which is why classic model training happens almost exclusively on them, as they offer the mix of large memory and good performance at high precision; this is an extremely different workload than run of the mill inference.\\n\\n## Cloud vs Owned and Operated\\n\\nWe find our customers anywhere on the spectrum of:\\n\\n- Open to Cloud, but want to control the stack: fine to use cloud instances, and Kamiwaza is cloud-deployable easily; in fact, we have a fully-scripted install that is tested on Microsoft Azure for releases, against Ubuntu 22.04LTS-Server\\n- Cloud ok for Test-Dev: They have use cases they want running on owned/operated hardware, but they will test on cloud instances\\n- Owned & Operated only: The enterprise wants to keep their entire flow private on hardware and software they control, for a variety of reasons\\n\\nObviously cloud, even using spot instances, can be a good way to do some early testing on certain models or use cases without a purchase."}]}}')}}]);