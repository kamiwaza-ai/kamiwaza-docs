"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[5741],{8070:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"use-cases/building-a-rag-pipeline","title":"Building a RAG Pipeline","description":"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza\'s core services.","source":"@site/versioned_docs/version-0.5.0/use-cases/building-a-rag-pipeline.md","sourceDirName":"use-cases","slug":"/use-cases/building-a-rag-pipeline","permalink":"/kamiwaza-docs/0.5.0/use-cases/building-a-rag-pipeline","draft":false,"unlisted":false,"tags":[],"version":"0.5.0","sidebarPosition":2,"frontMatter":{"id":"building-a-rag-pipeline","title":"Building a RAG Pipeline","sidebar_position":2},"sidebar":"mainSidebar","previous":{"title":"Use Cases","permalink":"/kamiwaza-docs/0.5.0/use-cases/"},"next":{"title":"Platform Overview","permalink":"/kamiwaza-docs/0.5.0/architecture/overview"}}');var s=t(74848),r=t(28453);const o={id:"building-a-rag-pipeline",title:"Building a RAG Pipeline",sidebar_position:2},l="Building a RAG Pipeline",a={},d=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Step 1: Deploy Required Models",id:"step-1-deploy-required-models",level:2},{value:"Deploy an Embedding Model",id:"deploy-an-embedding-model",level:3},{value:"Deploy a Language Model",id:"deploy-a-language-model",level:3},{value:"Step 2: Document Ingestion Pipeline",id:"step-2-document-ingestion-pipeline",level:2},{value:"Document Processing Script",id:"document-processing-script",level:3},{value:"Step 3: Implement Retrieval and Generation",id:"step-3-implement-retrieval-and-generation",level:2},{value:"Step 4: Example Queries",id:"step-4-example-queries",level:2},{value:"Step 5: Production Considerations",id:"step-5-production-considerations",level:2},{value:"Resource Management",id:"resource-management",level:3},{value:"Clean up",id:"clean-up",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Document Processing",id:"document-processing",level:3},{value:"Vector Search Optimization",id:"vector-search-optimization",level:3},{value:"LLM Integration",id:"llm-integration",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Key Benefits of This SDK-Based Approach",id:"key-benefits-of-this-sdk-based-approach",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"building-a-rag-pipeline",children:"Building a RAG Pipeline"})}),"\n",(0,s.jsx)(n.p,{children:"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza's core services."}),"\n",(0,s.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this guide, you'll have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Document ingestion system"})," that processes various file formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding pipeline"})," that converts text to vector representations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector search system"})," for finding relevant context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM integration"})," that generates responses using retrieved context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web interface"})," for querying your documents"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Kamiwaza installed and running (",(0,s.jsx)(n.a,{href:"../installation/installation_process",children:"Installation Guide"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"At least 16GB of available RAM"}),"\n",(0,s.jsx)(n.li,{children:"Sample documents (markdown format) to process"}),"\n",(0,s.jsx)(n.li,{children:"Basic familiarity with Python (for SDK examples)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:"A RAG pipeline consists of four main components:"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[Documents] --\x3e B[Ingestion & Chunking]\n    B --\x3e C[Embedding Generation]\n    C --\x3e D[Vector Storage]\n    D --\x3e E[Similarity Search]\n    E --\x3e F[LLM Generation]\n    F --\x3e G[Response]"}),"\n",(0,s.jsx)(n.h2,{id:"step-1-deploy-required-models",children:"Step 1: Deploy Required Models"}),"\n",(0,s.jsx)(n.p,{children:"First, we'll deploy an embedding model for vectorizing text and a language model for generating responses."}),"\n",(0,s.jsx)(n.h3,{id:"deploy-an-embedding-model",children:"Deploy an Embedding Model"}),"\n",(0,s.jsx)(n.p,{children:"The embedding model will be automatically loaded when you create an embedder - no manual deployment needed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from kamiwaza_client import KamiwazaClient\n\nclient = KamiwazaClient(base_url="http://localhost:7777/api/")\n\n# The embedding model will be automatically loaded when you create an embedder\n# This happens seamlessly in the background\nembedder = client.embedding.get_embedder(\n    model="BAAI/bge-base-en-v1.5",\n    provider_type="huggingface_embedding"\n)\n\nprint("\u2705 Embedding model ready for use")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"deploy-a-language-model",children:"Deploy a Language Model"}),"\n",(0,s.jsx)(n.p,{children:"Deploy a language model using Kamiwaza for response generation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from kamiwaza_client import KamiwazaClient\n\nclient = KamiwazaClient(base_url="http://localhost:7777/api/")\n\n# Search for a suitable language model\nmodel_repo = "Qwen/Qwen3-0.6B-GGUF"  # Small efficient model\nmodels = client.models.search_models(model_repo, exact=True)\nprint(f"Found model: {models[0]}")\n\n# Download the model (this may take a few minutes)\nprint("Downloading model...")\nclient.models.initiate_model_download(model_repo)\nclient.models.wait_for_download(model_repo)\nprint("\u2705 Model download complete")\n\n# Deploy the model\nprint("Deploying model...")\ndeployment_id = client.serving.deploy_model(repo_id=model_repo)\nprint(f"\u2705 Model deployed with ID: {deployment_id}")\n\n# Get OpenAI-compatible client for the deployed model\nopenai_client = client.openai.get_client(repo_id=model_repo)\nprint("\u2705 OpenAI-compatible client ready")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Check Deployment Status"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# List active deployments to verify\ndeployments = client.serving.list_active_deployments()\nfor deployment in deployments:\n    print(f"\u2705 {deployment.m_name} is {deployment.status}")\n    print(f"   Endpoint: {deployment.endpoint}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-2-document-ingestion-pipeline",children:"Step 2: Document Ingestion Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Now we'll create a pipeline to process documents, chunk them, and generate embeddings."}),"\n",(0,s.jsx)(n.h3,{id:"document-processing-script",children:"Document Processing Script"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\nfrom pathlib import Path\nfrom typing import List, Dict\nfrom kamiwaza_client import KamiwazaClient\n\nclass RAGPipeline:\n    def __init__(self, base_url="http://localhost:7777/api/"):\n        self.client = KamiwazaClient(base_url=base_url)\n        self.embedding_model = "BAAI/bge-base-en-v1.5"  # Use a proven working model\n        self.collection_name = "documents"\n        \n        # Initialize global embedder to prevent cleanup between operations\n        self.embedder = self.client.embedding.get_embedder(\n            model=self.embedding_model,\n            provider_type="huggingface_embedding"\n        )\n        print(f"\u2705 RAG Pipeline initialized with model: {self.embedding_model}")\n        \n    def add_documents_to_catalog(self, filepaths: List[str]) -> List:\n        """Add documents to the Kamiwaza catalog."""\n        datasets = []\n        \n        for filepath in filepaths:\n            try:\n                # Create dataset for each file\n                dataset = self.client.catalog.create_dataset(\n                    dataset_name=filepath,\n                    platform="file",\n                    environment="PROD",\n                    description=f"RAG document: {Path(filepath).name}"\n                )\n                \n                if dataset.urn:\n                    datasets.append(dataset)\n                    print(f"\u2705 Added to catalog: {Path(filepath).name}")\n                    \n            except Exception as e:\n                print(f"\u274c Error adding {filepath}: {str(e)}")\n        \n        return datasets\n    \n    def process_document(self, file_path: str):\n        """Process a single document: read, chunk, embed, and store."""\n        doc_path = Path(file_path)\n        \n        if not doc_path.exists():\n            raise FileNotFoundError(f"File not found: {doc_path}")\n        \n        # Read document content\n        with open(doc_path, \'r\', encoding=\'utf-8\') as f:\n            content = f.read()\n        \n        print(f"\ud83d\udcc4 Processing document: {doc_path.name}")\n        print(f"   - Size: {len(content)} characters")\n        \n        # Chunk the document using SDK\n        chunks = self.embedder.chunk_text(\n            text=content,\n            max_length=1024,  # Token-based chunking\n            overlap=102       # 10% overlap\n        )\n        print(f"   - Created {len(chunks)} chunks")\n        \n        # Generate embeddings for all chunks\n        embeddings = self.embedder.embed_chunks(chunks)\n        print(f"   - Generated {len(embeddings)} embeddings")\n        \n        # Prepare metadata for each chunk\n        metadata_list = []\n        for i, chunk in enumerate(chunks):\n            # Truncate chunk text if needed to fit storage limits\n            chunk_text = chunk[:900] + "..." if len(chunk) > 900 else chunk\n            \n            metadata = {\n                # Required autofields\n                "model_name": self.embedding_model,\n                "source": str(doc_path),\n                "offset": i,\n                "filename": doc_path.name,\n                \n                # Custom fields for better search\n                "chunk_text": chunk_text,  # Store the actual text\n                "chunk_index": i,\n                "chunk_size": len(chunk),\n                "document_title": doc_path.stem\n            }\n            metadata_list.append(metadata)\n        \n        # Define custom fields for the collection schema\n        field_list = [\n            ("chunk_text", "str"),\n            ("chunk_index", "int"), \n            ("chunk_size", "int"),\n            ("document_title", "str")\n        ]\n        \n        # Insert vectors using SDK\n        self.client.vectordb.insert(\n            vectors=embeddings,\n            metadata=metadata_list,\n            collection_name=self.collection_name,\n            field_list=field_list\n        )\n        \n        print(f"\u2705 Successfully stored {len(chunks)} chunks in collection \'{self.collection_name}\'")\n        return len(chunks)\n\n# Usage\npipeline = RAGPipeline()\n\n# Example: Process documents\nDOCUMENT_PATHS = [\n    "./docs/intro.md",  \n    "./docs/models/overview.md",\n    "./docs/architecture/overview.md",\n    "./docs/architecture/architecture.md",\n    "./docs/architecture/components.md"\n    # Add more documents as needed\n]\n\n# Optional: Add to catalog first\ndatasets = pipeline.add_documents_to_catalog(DOCUMENT_PATHS)\n\n# Process each document\ntotal_chunks = 0\nfor doc_path in DOCUMENT_PATHS:\n    try:\n        chunks = pipeline.process_document(doc_path)\n        total_chunks += chunks\n    except Exception as e:\n        print(f"\u274c Error processing {doc_path}: {str(e)}")\n\nprint(f"\\n\ud83c\udf89 Total chunks processed: {total_chunks}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-3-implement-retrieval-and-generation",children:"Step 3: Implement Retrieval and Generation"}),"\n",(0,s.jsx)(n.p,{children:"Now we'll create the query interface that retrieves relevant documents and generates responses."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from typing import List, Dict\nfrom kamiwaza_client import KamiwazaClient\n\nclass RAGQuery:\n    def __init__(self, base_url="http://localhost:7777/api/", chat_model_repo="Qwen/Qwen3-0.6B-GGUF"):\n        self.client = KamiwazaClient(base_url=base_url)\n        self.embedding_model = "BAAI/bge-base-en-v1.5"\n        self.chat_model_repo = chat_model_repo\n        self.collection_name = "documents"\n        \n        # Initialize embedder for query processing\n        self.embedder = self.client.embedding.get_embedder(\n            model=self.embedding_model,\n            provider_type="huggingface_embedding"\n        )\n        \n        # Get OpenAI-compatible client for the deployed chat model\n        try:\n            self.openai_client = self.client.openai.get_client(repo_id=self.chat_model_repo)\n            print(f"\u2705 RAG Query system initialized with chat model: {self.chat_model_repo}")\n        except Exception as e:\n            print(f"\u26a0\ufe0f Warning: Could not initialize chat model client: {e}")\n            print(f"   Make sure the model {self.chat_model_repo} is deployed")\n            self.openai_client = None\n    \n    def semantic_search(self, query: str, limit: int = 5) -> List[Dict]:\n        """Perform semantic search on the document collection."""\n        print(f"\ud83d\udd0d Searching for: \'{query}\'")\n        print(f"   - Collection: {self.collection_name}")\n        print(f"   - Max results: {limit}")\n        \n        # Generate embedding for the query\n        query_embedding = self.embedder.create_embedding(query).embedding\n        \n        # Perform vector search using SDK\n        results = self.client.vectordb.search(\n            query_vector=query_embedding,\n            collection_name=self.collection_name,\n            limit=limit,\n            output_fields=[\n                "source", "offset", "filename", "model_name", \n                "chunk_text", "chunk_index", "chunk_size", "document_title"\n            ]\n        )\n        \n        print(f"\u2705 Found {len(results)} relevant chunks")\n        return results\n    \n    def format_context(self, search_results: List[Dict]) -> str:\n        """Format search results into context for LLM."""\n        context_parts = []\n        \n        for result in search_results:\n            # Extract metadata\n            if hasattr(result, \'metadata\'):\n                metadata = result.metadata\n            elif isinstance(result, dict) and \'metadata\' in result:\n                metadata = result[\'metadata\']\n            else:\n                metadata = {}\n            \n            # Get chunk text and source info\n            chunk_text = metadata.get(\'chunk_text\', \'\')\n            filename = metadata.get(\'filename\', \'Unknown\')\n            document_title = metadata.get(\'document_title\', filename)\n            \n            if chunk_text:\n                context_parts.append(f"Document: {document_title}\\n{chunk_text}")\n        \n        return "\\n\\n".join(context_parts)\n    \n    def generate_response(self, query: str, context: str) -> str:\n        """Generate response using retrieved context and deployed Kamiwaza model."""\n        if not self.openai_client:\n            return f"[Error: Chat model not available. Please deploy {self.chat_model_repo} first]"\n        \n        prompt = f"""Based on the following context, answer the user\'s question. If the context doesn\'t contain enough information to answer the question, say so.\n\n        Context:\n        {context}\n\n        Question: {query}\n\n        Answer:"""\n        \n        try:\n            # Use the deployed Kamiwaza model via OpenAI-compatible interface\n            response = self.openai_client.chat.completions.create(\n                messages=[\n                    {"role": "user", "content": prompt}\n                ],\n                model="model",  # Use "model" as the model name for Kamiwaza OpenAI interface\n                max_tokens=500,\n                temperature=0.7,\n                stream=False\n            )\n            \n            return response.choices[0].message.content\n        \n        except Exception as e:\n            return f"[Error generating response: {str(e)}]"\n    \n    def query(self, user_question: str, limit: int = 5) -> Dict:\n        """Complete RAG query pipeline."""\n        print(f"\ud83e\udd16 Processing RAG query: {user_question}")\n        \n        # Search for relevant documents\n        search_results = self.semantic_search(user_question, limit=limit)\n        \n        # Format context for LLM\n        context = self.format_context(search_results)\n        \n        # Generate response (you\'ll need to implement LLM integration)\n        response = self.generate_response(user_question, context)\n        \n        # Prepare sources information\n        sources = []\n        for result in search_results:\n            metadata = result.metadata if hasattr(result, \'metadata\') else result.get(\'metadata\', {})\n            score = result.score if hasattr(result, \'score\') else result.get(\'score\', 0.0)\n            \n            sources.append({\n                \'filename\': metadata.get(\'filename\', \'Unknown\'),\n                \'document_title\': metadata.get(\'document_title\', \'\'),\n                \'chunk_index\': metadata.get(\'chunk_index\', 0),\n                \'score\': score\n            })\n        \n        return {\n            \'question\': user_question,\n            \'answer\': response,\n            \'context\': context,\n            \'sources\': sources,\n            \'num_results\': len(search_results)\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-4-example-queries",children:"Step 4: Example Queries"}),"\n",(0,s.jsx)(n.p,{children:"Let's test the RAG system with an example query to demonstrate its capabilities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example query to test your RAG system\nrag = RAGQuery()\n\nquery_response = rag.query(\"What is one cool thing about Kamiwaza?\")\n\nprint(query_response['answer'])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-5-production-considerations",children:"Step 5: Production Considerations"}),"\n",(0,s.jsx)(n.p,{children:"When moving your RAG system to production, consider these key aspects:"}),"\n",(0,s.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Monitor system resources and manage deployments\ndef monitor_system_health():\n    """Monitor system health and resource usage."""\n    client = KamiwazaClient(base_url="http://localhost:7777/api/")\n    \n    # Check active deployments\n    deployments = client.serving.list_active_deployments()\n    print(f"\ud83d\udcca Active Deployments: {len(deployments)}")\n    \n    for deployment in deployments:\n        print(f"   - {deployment.m_name}: {deployment.status}")\n        print(f"     Endpoint: {deployment.endpoint}")\n    \n    # Check vector collections\n    collections = client.vectordb.list_collections()\n    print(f"\ud83d\udcda Vector Collections: {collections}")\n\n# Run health check\nmonitor_system_health()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"clean-up",children:"Clean up"}),"\n",(0,s.jsx)(n.p,{children:"When done, stop the model deployment to free resources"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def cleanup_rag_system(chat_model_repo="Qwen/Qwen3-0.6B-GGUF"):\n    """Stop model deployments to free up resources."""\n    client = KamiwazaClient(base_url="http://localhost:7777/api/")\n    \n    try:\n        success = client.serving.stop_deployment(repo_id=chat_model_repo)\n        if success:\n            print(f"\u2705 Stopped deployment for {chat_model_repo}")\n        else:\n            print(f"\u274c Failed to stop deployment for {chat_model_repo}")\n    except Exception as e:\n        print(f"\u274c Error stopping deployment: {e}")\n\ncleanup_rag_system()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"document-processing",children:"Document Processing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chunk Size"}),": Keep chunks between 200-800 tokens for optimal retrieval"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overlap"}),": Add 50-100 token overlap between chunks to preserve context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metadata"}),": Include rich metadata (source, date, author) for filtering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Clean text, remove headers/footers, handle special characters"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vector-search-optimization",children:"Vector Search Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Index Tuning"}),": Adjust index parameters based on collection size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reranking"}),": Use a reranking model for better result quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hybrid Search"}),": Combine vector search with keyword matching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Filtering"}),": Use metadata filters to narrow search scope"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Window"}),": Stay within model's context limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"}),": Design clear, specific system prompts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temperature"}),": Use lower values (0.1-0.3) for factual responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Citations"}),": Always include source attribution in responses"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Poor Retrieval Quality"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check embedding model performance on your domain"}),"\n",(0,s.jsx)(n.li,{children:"Adjust chunk size and overlap"}),"\n",(0,s.jsx)(n.li,{children:"Try different similarity metrics (cosine vs. dot product)"}),"\n",(0,s.jsx)(n.li,{children:"Consider domain-specific fine-tuning"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Slow Query Performance"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize vector index parameters"}),"\n",(0,s.jsxs)(n.li,{children:["Reduce ",(0,s.jsx)(n.code,{children:"top_k"})," in retrieval"]}),"\n",(0,s.jsx)(n.li,{children:"Use GPU acceleration for embeddings"}),"\n",(0,s.jsx)(n.li,{children:"Implement result caching"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inaccurate Responses"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Improve prompt engineering"}),"\n",(0,s.jsx)(n.li,{children:"Increase retrieved context size"}),"\n",(0,s.jsx)(n.li,{children:"Use a larger/better language model"}),"\n",(0,s.jsx)(n.li,{children:"Add response validation logic"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Now that you have a working RAG pipeline with Kamiwaza-deployed models:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Try Different Models"}),": Experiment with larger models like ",(0,s.jsx)(n.code,{children:"Qwen/Qwen3-32B-GGUF"})," for better response quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize Retrieval"}),": Experiment with different embedding models, chunk sizes, and similarity thresholds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add Streaming"}),": Use ",(0,s.jsx)(n.code,{children:"stream=True"})," in the chat completions for real-time response streaming"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement Reranking"}),": Add semantic reranking for better result quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Your System"}),": Deploy multiple model instances using Kamiwaza's ",(0,s.jsx)(n.a,{href:"../architecture/overview",children:"distributed architecture"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Performance"}),": Add logging and metrics to track query performance and model usage"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-benefits-of-this-sdk-based-approach",children:"Key Benefits of This SDK-Based Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simplified Integration"}),": No need for manual HTTP requests - the SDK handles all API communication"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic Schema Management"}),": Collections and schemas are created automatically based on your data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Built-in Best Practices"}),": The SDK incorporates proven patterns for chunking, embedding, and vector storage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Catalog Integration"}),": Documents are managed through Kamiwaza's catalog system for better organization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Production Ready"}),": SDK handles error cases, retries, and connection management"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Your RAG pipeline is now ready to answer questions using your own documents! The combination of Kamiwaza's SDK with proper document processing creates a robust foundation for production RAG applications."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);