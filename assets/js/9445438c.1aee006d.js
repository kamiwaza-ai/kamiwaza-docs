"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[5645],{45602:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"use-cases/building-a-rag-pipeline","title":"Building a RAG Pipeline","description":"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza\'s core services.","source":"@site/versioned_docs/version-0.5.1/use-cases/building-a-rag-pipeline.md","sourceDirName":"use-cases","slug":"/use-cases/building-a-rag-pipeline","permalink":"/0.5.1/use-cases/building-a-rag-pipeline","draft":false,"unlisted":false,"tags":[],"version":"0.5.1","sidebarPosition":2,"frontMatter":{"id":"building-a-rag-pipeline","title":"Building a RAG Pipeline","sidebar_position":2},"sidebar":"mainSidebar","previous":{"title":"Use Cases","permalink":"/0.5.1/use-cases/"},"next":{"title":"Platform Overview","permalink":"/0.5.1/architecture/overview"}}');var i=r(74848),s=r(28453);const o={id:"building-a-rag-pipeline",title:"Building a RAG Pipeline",sidebar_position:2},l="Building a RAG Pipeline",a={},d=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Step 1: Deploy Required Models",id:"step-1-deploy-required-models",level:2},{value:"Deploy an Embedding Model",id:"deploy-an-embedding-model",level:3},{value:"Deploy a Language Model",id:"deploy-a-language-model",level:3},{value:"Step 2: Document Ingestion Pipeline",id:"step-2-document-ingestion-pipeline",level:2},{value:"Document Processing Script",id:"document-processing-script",level:3},{value:"Step 3: Implement Retrieval and Generation",id:"step-3-implement-retrieval-and-generation",level:2},{value:"Step 4: Example Queries",id:"step-4-example-queries",level:2},{value:"Step 5: Production Considerations",id:"step-5-production-considerations",level:2},{value:"Resource Management",id:"resource-management",level:3},{value:"Clean up",id:"clean-up",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Document Processing",id:"document-processing",level:3},{value:"Vector Search Optimization",id:"vector-search-optimization",level:3},{value:"LLM Integration",id:"llm-integration",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Key Benefits of This SDK-Based Approach",id:"key-benefits-of-this-sdk-based-approach",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"building-a-rag-pipeline",children:"Building a RAG Pipeline"})}),"\n",(0,i.jsx)(n.p,{children:"Retrieval-Augmented Generation (RAG) combines the power of large language models with your own documents and data to provide accurate, contextual responses. This guide walks you through building a complete RAG pipeline using Kamiwaza's core services."}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this guide, you'll have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document ingestion system"})," that processes various file formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding pipeline"})," that converts text to vector representations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vector search system"})," for finding relevant context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM integration"})," that generates responses using retrieved context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Web interface"})," for querying your documents"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Kamiwaza installed and running (",(0,i.jsx)(n.a,{href:"../installation/installation_process",children:"Installation Guide"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"At least 16GB of available RAM"}),"\n",(0,i.jsx)(n.li,{children:"Sample documents (markdown format) to process"}),"\n",(0,i.jsx)(n.li,{children:"Basic familiarity with Python (for SDK examples)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(n.p,{children:"A RAG pipeline consists of four main components:"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\r\n    A[Documents] --\x3e B[Ingestion & Chunking]\r\n    B --\x3e C[Embedding Generation]\r\n    C --\x3e D[Vector Storage]\r\n    D --\x3e E[Similarity Search]\r\n    E --\x3e F[LLM Generation]\r\n    F --\x3e G[Response]"}),"\n",(0,i.jsx)(n.h2,{id:"step-1-deploy-required-models",children:"Step 1: Deploy Required Models"}),"\n",(0,i.jsx)(n.p,{children:"First, we'll deploy an embedding model for vectorizing text and a language model for generating responses."}),"\n",(0,i.jsx)(n.h3,{id:"deploy-an-embedding-model",children:"Deploy an Embedding Model"}),"\n",(0,i.jsx)(n.p,{children:"The embedding model will be automatically loaded when you create an embedder - no manual deployment needed:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from kamiwaza_client import KamiwazaClient\r\n\r\nclient = KamiwazaClient(base_url="http://localhost:7777/api/")\r\n\r\n# The embedding model will be automatically loaded when you create an embedder\r\n# This happens seamlessly in the background\r\nembedder = client.embedding.get_embedder(\r\n    model="BAAI/bge-base-en-v1.5",\r\n    provider_type="huggingface_embedding"\r\n)\r\n\r\nprint("\u2705 Embedding model ready for use")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"deploy-a-language-model",children:"Deploy a Language Model"}),"\n",(0,i.jsx)(n.p,{children:"Deploy a language model using Kamiwaza for response generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from kamiwaza_client import KamiwazaClient\r\n\r\nclient = KamiwazaClient(base_url="http://localhost:7777/api/")\r\n\r\n# Search for a suitable language model\r\nmodel_repo = "Qwen/Qwen3-0.6B-GGUF"  # Small efficient model\r\nmodels = client.models.search_models(model_repo, exact=True)\r\nprint(f"Found model: {models[0]}")\r\n\r\n# Download the model (this may take a few minutes)\r\nprint("Downloading model...")\r\nclient.models.initiate_model_download(model_repo)\r\nclient.models.wait_for_download(model_repo)\r\nprint("\u2705 Model download complete")\r\n\r\n# Deploy the model\r\nprint("Deploying model...")\r\ndeployment_id = client.serving.deploy_model(repo_id=model_repo)\r\nprint(f"\u2705 Model deployed with ID: {deployment_id}")\r\n\r\n# Get OpenAI-compatible client for the deployed model\r\nopenai_client = client.openai.get_client(repo_id=model_repo)\r\nprint("\u2705 OpenAI-compatible client ready")\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Check Deployment Status"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# List active deployments to verify\r\ndeployments = client.serving.list_active_deployments()\r\nfor deployment in deployments:\r\n    print(f"\u2705 {deployment.m_name} is {deployment.status}")\r\n    print(f"   Endpoint: {deployment.endpoint}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-2-document-ingestion-pipeline",children:"Step 2: Document Ingestion Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Now we'll create a pipeline to process documents, chunk them, and generate embeddings."}),"\n",(0,i.jsx)(n.h3,{id:"document-processing-script",children:"Document Processing Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\r\nfrom pathlib import Path\r\nfrom typing import List, Dict\r\nfrom kamiwaza_client import KamiwazaClient\r\n\r\nclass RAGPipeline:\r\n    def __init__(self, base_url="http://localhost:7777/api/"):\r\n        self.client = KamiwazaClient(base_url=base_url)\r\n        self.embedding_model = "BAAI/bge-base-en-v1.5"  # Use a proven working model\r\n        self.collection_name = "documents"\r\n        \r\n        # Initialize global embedder to prevent cleanup between operations\r\n        self.embedder = self.client.embedding.get_embedder(\r\n            model=self.embedding_model,\r\n            provider_type="huggingface_embedding"\r\n        )\r\n        print(f"\u2705 RAG Pipeline initialized with model: {self.embedding_model}")\r\n        \r\n    def add_documents_to_catalog(self, filepaths: List[str]) -> List:\r\n        """Add documents to the Kamiwaza catalog."""\r\n        datasets = []\r\n        \r\n        for filepath in filepaths:\r\n            try:\r\n                # Create dataset for each file\r\n                dataset = self.client.catalog.create_dataset(\r\n                    dataset_name=filepath,\r\n                    platform="file",\r\n                    environment="PROD",\r\n                    description=f"RAG document: {Path(filepath).name}"\r\n                )\r\n                \r\n                if dataset.urn:\r\n                    datasets.append(dataset)\r\n                    print(f"\u2705 Added to catalog: {Path(filepath).name}")\r\n                    \r\n            except Exception as e:\r\n                print(f"\u274c Error adding {filepath}: {str(e)}")\r\n        \r\n        return datasets\r\n    \r\n    def process_document(self, file_path: str):\r\n        """Process a single document: read, chunk, embed, and store."""\r\n        doc_path = Path(file_path)\r\n        \r\n        if not doc_path.exists():\r\n            raise FileNotFoundError(f"File not found: {doc_path}")\r\n        \r\n        # Read document content\r\n        with open(doc_path, \'r\', encoding=\'utf-8\') as f:\r\n            content = f.read()\r\n        \r\n        print(f"\ud83d\udcc4 Processing document: {doc_path.name}")\r\n        print(f"   - Size: {len(content)} characters")\r\n        \r\n        # Chunk the document using SDK\r\n        chunks = self.embedder.chunk_text(\r\n            text=content,\r\n            max_length=1024,  # Token-based chunking\r\n            overlap=102       # 10% overlap\r\n        )\r\n        print(f"   - Created {len(chunks)} chunks")\r\n        \r\n        # Generate embeddings for all chunks\r\n        embeddings = self.embedder.embed_chunks(chunks)\r\n        print(f"   - Generated {len(embeddings)} embeddings")\r\n        \r\n        # Prepare metadata for each chunk\r\n        metadata_list = []\r\n        for i, chunk in enumerate(chunks):\r\n            # Truncate chunk text if needed to fit storage limits\r\n            chunk_text = chunk[:900] + "..." if len(chunk) > 900 else chunk\r\n            \r\n            metadata = {\r\n                # Required autofields\r\n                "model_name": self.embedding_model,\r\n                "source": str(doc_path),\r\n                "offset": i,\r\n                "filename": doc_path.name,\r\n                \r\n                # Custom fields for better search\r\n                "chunk_text": chunk_text,  # Store the actual text\r\n                "chunk_index": i,\r\n                "chunk_size": len(chunk),\r\n                "document_title": doc_path.stem\r\n            }\r\n            metadata_list.append(metadata)\r\n        \r\n        # Define custom fields for the collection schema\r\n        field_list = [\r\n            ("chunk_text", "str"),\r\n            ("chunk_index", "int"), \r\n            ("chunk_size", "int"),\r\n            ("document_title", "str")\r\n        ]\r\n        \r\n        # Insert vectors using SDK\r\n        self.client.vectordb.insert(\r\n            vectors=embeddings,\r\n            metadata=metadata_list,\r\n            collection_name=self.collection_name,\r\n            field_list=field_list\r\n        )\r\n        \r\n        print(f"\u2705 Successfully stored {len(chunks)} chunks in collection \'{self.collection_name}\'")\r\n        return len(chunks)\r\n\r\n# Usage\r\npipeline = RAGPipeline()\r\n\r\n# Example: Process documents\r\nDOCUMENT_PATHS = [\r\n    "./docs/intro.md",  \r\n    "./docs/models/overview.md",\r\n    "./docs/architecture/overview.md",\r\n    "./docs/architecture/architecture.md",\r\n    "./docs/architecture/components.md"\r\n    # Add more documents as needed\r\n]\r\n\r\n# Optional: Add to catalog first\r\ndatasets = pipeline.add_documents_to_catalog(DOCUMENT_PATHS)\r\n\r\n# Process each document\r\ntotal_chunks = 0\r\nfor doc_path in DOCUMENT_PATHS:\r\n    try:\r\n        chunks = pipeline.process_document(doc_path)\r\n        total_chunks += chunks\r\n    except Exception as e:\r\n        print(f"\u274c Error processing {doc_path}: {str(e)}")\r\n\r\nprint(f"\\n\ud83c\udf89 Total chunks processed: {total_chunks}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-3-implement-retrieval-and-generation",children:"Step 3: Implement Retrieval and Generation"}),"\n",(0,i.jsx)(n.p,{children:"Now we'll create the query interface that retrieves relevant documents and generates responses."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import List, Dict\r\nfrom kamiwaza_client import KamiwazaClient\r\n\r\nclass RAGQuery:\r\n    def __init__(self, base_url="http://localhost:7777/api/", chat_model_repo="Qwen/Qwen3-0.6B-GGUF"):\r\n        self.client = KamiwazaClient(base_url=base_url)\r\n        self.embedding_model = "BAAI/bge-base-en-v1.5"\r\n        self.chat_model_repo = chat_model_repo\r\n        self.collection_name = "documents"\r\n        \r\n        # Initialize embedder for query processing\r\n        self.embedder = self.client.embedding.get_embedder(\r\n            model=self.embedding_model,\r\n            provider_type="huggingface_embedding"\r\n        )\r\n        \r\n        # Get OpenAI-compatible client for the deployed chat model\r\n        try:\r\n            self.openai_client = self.client.openai.get_client(repo_id=self.chat_model_repo)\r\n            print(f"\u2705 RAG Query system initialized with chat model: {self.chat_model_repo}")\r\n        except Exception as e:\r\n            print(f"\u26a0\ufe0f Warning: Could not initialize chat model client: {e}")\r\n            print(f"   Make sure the model {self.chat_model_repo} is deployed")\r\n            self.openai_client = None\r\n    \r\n    def semantic_search(self, query: str, limit: int = 5) -> List[Dict]:\r\n        """Perform semantic search on the document collection."""\r\n        print(f"\ud83d\udd0d Searching for: \'{query}\'")\r\n        print(f"   - Collection: {self.collection_name}")\r\n        print(f"   - Max results: {limit}")\r\n        \r\n        # Generate embedding for the query\r\n        query_embedding = self.embedder.create_embedding(query).embedding\r\n        \r\n        # Perform vector search using SDK\r\n        results = self.client.vectordb.search(\r\n            query_vector=query_embedding,\r\n            collection_name=self.collection_name,\r\n            limit=limit,\r\n            output_fields=[\r\n                "source", "offset", "filename", "model_name", \r\n                "chunk_text", "chunk_index", "chunk_size", "document_title"\r\n            ]\r\n        )\r\n        \r\n        print(f"\u2705 Found {len(results)} relevant chunks")\r\n        return results\r\n    \r\n    def format_context(self, search_results: List[Dict]) -> str:\r\n        """Format search results into context for LLM."""\r\n        context_parts = []\r\n        \r\n        for result in search_results:\r\n            # Extract metadata\r\n            if hasattr(result, \'metadata\'):\r\n                metadata = result.metadata\r\n            elif isinstance(result, dict) and \'metadata\' in result:\r\n                metadata = result[\'metadata\']\r\n            else:\r\n                metadata = {}\r\n            \r\n            # Get chunk text and source info\r\n            chunk_text = metadata.get(\'chunk_text\', \'\')\r\n            filename = metadata.get(\'filename\', \'Unknown\')\r\n            document_title = metadata.get(\'document_title\', filename)\r\n            \r\n            if chunk_text:\r\n                context_parts.append(f"Document: {document_title}\\n{chunk_text}")\r\n        \r\n        return "\\n\\n".join(context_parts)\r\n    \r\n    def generate_response(self, query: str, context: str) -> str:\r\n        """Generate response using retrieved context and deployed Kamiwaza model."""\r\n        if not self.openai_client:\r\n            return f"[Error: Chat model not available. Please deploy {self.chat_model_repo} first]"\r\n        \r\n        prompt = f"""Based on the following context, answer the user\'s question. If the context doesn\'t contain enough information to answer the question, say so.\r\n\r\n        Context:\r\n        {context}\r\n\r\n        Question: {query}\r\n\r\n        Answer:"""\r\n        \r\n        try:\r\n            # Use the deployed Kamiwaza model via OpenAI-compatible interface\r\n            response = self.openai_client.chat.completions.create(\r\n                messages=[\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                model="model",  # Use "model" as the model name for Kamiwaza OpenAI interface\r\n                max_tokens=500,\r\n                temperature=0.7,\r\n                stream=False\r\n            )\r\n            \r\n            return response.choices[0].message.content\r\n        \r\n        except Exception as e:\r\n            return f"[Error generating response: {str(e)}]"\r\n    \r\n    def query(self, user_question: str, limit: int = 5) -> Dict:\r\n        """Complete RAG query pipeline."""\r\n        print(f"\ud83e\udd16 Processing RAG query: {user_question}")\r\n        \r\n        # Search for relevant documents\r\n        search_results = self.semantic_search(user_question, limit=limit)\r\n        \r\n        # Format context for LLM\r\n        context = self.format_context(search_results)\r\n        \r\n        # Generate response (you\'ll need to implement LLM integration)\r\n        response = self.generate_response(user_question, context)\r\n        \r\n        # Prepare sources information\r\n        sources = []\r\n        for result in search_results:\r\n            metadata = result.metadata if hasattr(result, \'metadata\') else result.get(\'metadata\', {})\r\n            score = result.score if hasattr(result, \'score\') else result.get(\'score\', 0.0)\r\n            \r\n            sources.append({\r\n                \'filename\': metadata.get(\'filename\', \'Unknown\'),\r\n                \'document_title\': metadata.get(\'document_title\', \'\'),\r\n                \'chunk_index\': metadata.get(\'chunk_index\', 0),\r\n                \'score\': score\r\n            })\r\n        \r\n        return {\r\n            \'question\': user_question,\r\n            \'answer\': response,\r\n            \'context\': context,\r\n            \'sources\': sources,\r\n            \'num_results\': len(search_results)\r\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-4-example-queries",children:"Step 4: Example Queries"}),"\n",(0,i.jsx)(n.p,{children:"Let's test the RAG system with an example query to demonstrate its capabilities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example query to test your RAG system\r\nrag = RAGQuery()\r\n\r\nquery_response = rag.query(\"What is one cool thing about Kamiwaza?\")\r\n\r\nprint(query_response['answer'])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-5-production-considerations",children:"Step 5: Production Considerations"}),"\n",(0,i.jsx)(n.p,{children:"When moving your RAG system to production, consider these key aspects:"}),"\n",(0,i.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Monitor system resources and manage deployments\r\ndef monitor_system_health():\r\n    """Monitor system health and resource usage."""\r\n    client = KamiwazaClient(base_url="http://localhost:7777/api/")\r\n    \r\n    # Check active deployments\r\n    deployments = client.serving.list_active_deployments()\r\n    print(f"\ud83d\udcca Active Deployments: {len(deployments)}")\r\n    \r\n    for deployment in deployments:\r\n        print(f"   - {deployment.m_name}: {deployment.status}")\r\n        print(f"     Endpoint: {deployment.endpoint}")\r\n    \r\n    # Check vector collections\r\n    collections = client.vectordb.list_collections()\r\n    print(f"\ud83d\udcda Vector Collections: {collections}")\r\n\r\n# Run health check\r\nmonitor_system_health()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"clean-up",children:"Clean up"}),"\n",(0,i.jsx)(n.p,{children:"When done, stop the model deployment to free resources"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def cleanup_rag_system(chat_model_repo="Qwen/Qwen3-0.6B-GGUF"):\r\n    """Stop model deployments to free up resources."""\r\n    client = KamiwazaClient(base_url="http://localhost:7777/api/")\r\n    \r\n    try:\r\n        success = client.serving.stop_deployment(repo_id=chat_model_repo)\r\n        if success:\r\n            print(f"\u2705 Stopped deployment for {chat_model_repo}")\r\n        else:\r\n            print(f"\u274c Failed to stop deployment for {chat_model_repo}")\r\n    except Exception as e:\r\n        print(f"\u274c Error stopping deployment: {e}")\r\n\r\ncleanup_rag_system()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"document-processing",children:"Document Processing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chunk Size"}),": Keep chunks between 200-800 tokens for optimal retrieval"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Overlap"}),": Add 50-100 token overlap between chunks to preserve context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Metadata"}),": Include rich metadata (source, date, author) for filtering"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preprocessing"}),": Clean text, remove headers/footers, handle special characters"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"vector-search-optimization",children:"Vector Search Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Index Tuning"}),": Adjust index parameters based on collection size"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reranking"}),": Use a reranking model for better result quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Search"}),": Combine vector search with keyword matching"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering"}),": Use metadata filters to narrow search scope"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Window"}),": Stay within model's context limits"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prompt Engineering"}),": Design clear, specific system prompts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temperature"}),": Use lower values (0.1-0.3) for factual responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Citations"}),": Always include source attribution in responses"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Poor Retrieval Quality"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Check embedding model performance on your domain"}),"\n",(0,i.jsx)(n.li,{children:"Adjust chunk size and overlap"}),"\n",(0,i.jsx)(n.li,{children:"Try different similarity metrics (cosine vs. dot product)"}),"\n",(0,i.jsx)(n.li,{children:"Consider domain-specific fine-tuning"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Slow Query Performance"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimize vector index parameters"}),"\n",(0,i.jsxs)(n.li,{children:["Reduce ",(0,i.jsx)(n.code,{children:"top_k"})," in retrieval"]}),"\n",(0,i.jsx)(n.li,{children:"Use GPU acceleration for embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Implement result caching"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Inaccurate Responses"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improve prompt engineering"}),"\n",(0,i.jsx)(n.li,{children:"Increase retrieved context size"}),"\n",(0,i.jsx)(n.li,{children:"Use a larger/better language model"}),"\n",(0,i.jsx)(n.li,{children:"Add response validation logic"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Now that you have a working RAG pipeline with Kamiwaza-deployed models:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Try Different Models"}),": Experiment with larger models like ",(0,i.jsx)(n.code,{children:"Qwen/Qwen3-32B-GGUF"})," for better response quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimize Retrieval"}),": Experiment with different embedding models, chunk sizes, and similarity thresholds"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add Streaming"}),": Use ",(0,i.jsx)(n.code,{children:"stream=True"})," in the chat completions for real-time response streaming"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement Reranking"}),": Add semantic reranking for better result quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale Your System"}),": Deploy multiple model instances using Kamiwaza's ",(0,i.jsx)(n.a,{href:"../architecture/overview",children:"distributed architecture"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor Performance"}),": Add logging and metrics to track query performance and model usage"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-benefits-of-this-sdk-based-approach",children:"Key Benefits of This SDK-Based Approach"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simplified Integration"}),": No need for manual HTTP requests - the SDK handles all API communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automatic Schema Management"}),": Collections and schemas are created automatically based on your data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Built-in Best Practices"}),": The SDK incorporates proven patterns for chunking, embedding, and vector storage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Catalog Integration"}),": Documents are managed through Kamiwaza's catalog system for better organization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Production Ready"}),": SDK handles error cases, retries, and connection management"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Your RAG pipeline is now ready to answer questions using your own documents! The combination of Kamiwaza's SDK with proper document processing creates a robust foundation for production RAG applications."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var t=r(96540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);