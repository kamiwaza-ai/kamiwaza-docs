"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[5558],{51808:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>i,default:()=>c,frontMatter:()=>d,metadata:()=>s,toc:()=>r});const s=JSON.parse('{"id":"models/gui-walkthrough","title":"GUI Walkthrough","description":"The following walkthrough is based on the Kamiwaza 1.0.0 user interface. We\'ll walk through navigating the Kamiwaza admin console to search for a model, download model files, and then deploy and use an inference endpoint.","source":"@site/versioned_docs/version-0.5.1/models/gui-walkthrough.md","sourceDirName":"models","slug":"/models/gui-walkthrough","permalink":"/0.5.1/models/gui-walkthrough","draft":false,"unlisted":false,"tags":[],"version":"0.5.1","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"mainSidebar","previous":{"title":"Deploying Models in Novice Mode","permalink":"/0.5.1/models/novice-mode"},"next":{"title":"Downloading Models","permalink":"/0.5.1/models/downloading-models"}}');var t=o(74848),l=o(28453);const d={sidebar_position:4},i="GUI Walkthrough",a={},r=[{value:"Step 1: Find and click the Models menu in the sidebar",id:"step-1-find-and-click-the-models-menu-in-the-sidebar",level:2},{value:"Step 2: Under Model Hub Search, type keywords for your desired model",id:"step-2-under-model-hub-search-type-keywords-for-your-desired-model",level:2},{value:"Step 3: Download models files from chosen model",id:"step-3-download-models-files-from-chosen-model",level:2},{value:"Step 4: Check the model files after the download",id:"step-4-check-the-model-files-after-the-download",level:2},{value:"Step 5: Deploy a model for serving",id:"step-5-deploy-a-model-for-serving",level:2},{value:"Step 6: Testing the deployed model endpoint",id:"step-6-testing-the-deployed-model-endpoint",level:2}];function h(e){const n={blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",p:"p",strong:"strong",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"gui-walkthrough",children:"GUI Walkthrough"})}),"\n",(0,t.jsxs)(n.p,{children:["The following walkthrough is based on the Kamiwaza 1.0.0 user interface. We'll walk through navigating the Kamiwaza admin console to ",(0,t.jsx)(n.strong,{children:"search"})," for a model, ",(0,t.jsx)(n.strong,{children:"download"})," model files, and then ",(0,t.jsx)(n.strong,{children:"deploy and use"})," an inference endpoint."]}),"\n",(0,t.jsx)(n.h2,{id:"step-1-find-and-click-the-models-menu-in-the-sidebar",children:"Step 1: Find and click the Models menu in the sidebar"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"The Kamiwaza console sidebar",src:o(25420).A+"",width:"1133",height:"669"})}),"\n",(0,t.jsx)(n.h2,{id:"step-2-under-model-hub-search-type-keywords-for-your-desired-model",children:"Step 2: Under Model Hub Search, type keywords for your desired model"}),"\n",(0,t.jsx)(n.p,{children:"In this example, I'm looking for a GGUF of Llama3.1 8B Instruct, so I type a few of its keywords to narrow results."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Model search bar",src:o(31075).A+"",width:"797",height:"428"})}),"\n",(0,t.jsxs)(n.p,{children:["Clicking the ",(0,t.jsx)(n.strong,{children:"Search"})," button will show the results, like this:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Model search results",src:o(68435).A+"",width:"1028",height:"635"})}),"\n",(0,t.jsx)(n.h2,{id:"step-3-download-models-files-from-chosen-model",children:"Step 3: Download models files from chosen model"}),"\n",(0,t.jsxs)(n.p,{children:["From the results, let's choose the Bartowski GGUF. Clicking the ",(0,t.jsx)(n.strong,{children:"Download"})," button for ",(0,t.jsx)(n.em,{children:"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF"})," results in the following:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Download files list",src:o(21130).A+"",width:"713",height:"996"})}),"\n",(0,t.jsx)(n.p,{children:"In this example, we are downloading GGUF models for a llamacpp inference engine deployment. Unlike normal Hugging Face models (safetensors), we only need to download one or a couple files - just the specific quantized model that we need."}),"\n",(0,t.jsxs)(n.p,{children:["For this example, we will uncheck everything and choose only the ",(0,t.jsx)(n.strong,{children:"Q8_0"})," variant. Click ",(0,t.jsx)(n.strong,{children:'Select/Unselect"'})," all to uncheck all files, then scroll down and click the file marked ",(0,t.jsx)(n.strong,{children:"Q8_0"})," (near the very bottom), and then click ",(0,t.jsx)(n.strong,{children:"Download Selected Files"})," to download the 8-bit quantized version of Llama 3.1 8B Instruct in GGUF format."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["NOTE: In normal Hugging Face models (non-GGUF), we usually need to download all files in order to serve a model (safetensors, config, tokenizer, etc), hence having everything pre-checked. We'd only need to scroll down and click the ",(0,t.jsx)(n.strong,{children:"Download Selected Files"})," button."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-4-check-the-model-files-after-the-download",children:"Step 4: Check the model files after the download"}),"\n",(0,t.jsx)(n.p,{children:"After the download, your Kamiwaza models console will look something like this:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Downloaded models",src:o(59415).A+"",width:"1118",height:"297"})}),"\n",(0,t.jsx)(n.p,{children:"Your downloaded model is displayed. To view the specific files downloaded for this model, click the name of the model in the list. You'll see a screen that looks like this:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Model details",src:o(78856).A+"",width:"1537",height:"806"})}),"\n",(0,t.jsxs)(n.p,{children:["You'll notice most of the files under ",(0,t.jsx)(n.strong,{children:"Model Files"})," have an empty status, since we didn't download them. Scroll down to see if our 8-bit quantized GGUF is present, though:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Model details - Q8_0 found",src:o(13726).A+"",width:"758",height:"390"})}),"\n",(0,t.jsx)(n.p,{children:'And yep, as expected, our Q8_0-quantized GGUF is there. Since it is already marked as "downloaded", that means we can deploy it already.'}),"\n",(0,t.jsx)(n.h2,{id:"step-5-deploy-a-model-for-serving",children:"Step 5: Deploy a model for serving"}),"\n",(0,t.jsxs)(n.p,{children:["We've already seen the ",(0,t.jsx)(n.strong,{children:"Deploy"})," button earlier, at the upper right area of the Model Details screen, under Model Configurations:\r\n",(0,t.jsx)(n.img,{alt:"Model details",src:o(78856).A+"",width:"1537",height:"806"})]}),"\n",(0,t.jsx)(n.p,{children:"Kamiwaza creates a default config for us when we download a model, to simplify the deployment experience."}),"\n",(0,t.jsxs)(n.p,{children:["Click ",(0,t.jsx)(n.strong,{children:'"Deploy"'})," to launch an instance. When the spinner finishes, click ",(0,t.jsx)(n.strong,{children:"Back to Model List"})," to go back to the main Models screen."]}),"\n",(0,t.jsx)(n.h2,{id:"step-6-testing-the-deployed-model-endpoint",children:"Step 6: Testing the deployed model endpoint"}),"\n",(0,t.jsx)(n.p,{children:"In the main Models screen, under Model Deployments, you will see a new entry for our recently deployed Llama 3.1 8B model:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Model endpoint list",src:o(25065).A+"",width:"1110",height:"235"})}),"\n",(0,t.jsx)(n.p,{children:'To test our model, click "Open Llamacpp Chat". Your browser will open a chat application running against the deployed model:'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Test chat",src:o(20252).A+"",width:"1119",height:"260"})}),"\n",(0,t.jsx)(n.p,{children:"Let's send a test message to validate that our LLM endpoint is working:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Test chat",src:o(20499).A+"",width:"999",height:"364"})}),"\n",(0,t.jsx)(n.p,{children:"Perfect! If you see a response like that, then our inference endpoint is working as expected."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"\ud83c\udf89 You've successfully deployed your first model on Kamiwaza! \ud83c\udf89"})})]})}function c(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},25420:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/01_sidebar-b933bb6e7adf03b77096847ca1eee4c3.png"},31075:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/02_search-6d00ed534d680d509ce8af4b7bb64c60.png"},68435:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/03_search_results-bbf2ef8270eb2acce52ce2e528e46806.png"},21130:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/04_download_list-b52c1fbf12ce099449381c996a70da20.png"},59415:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/05_model_downloaded-5c970105a54f2f6590cf4cb2c562769c.png"},78856:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/06_model_files_1-6a044593dedb80bbd09a5dd656ba567b.png"},13726:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/07_model_files_2-bd8816e0a9b53255db6f81bed1caa768.png"},25065:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/08_model_endpoint_list-3230a6ae0d30713b628ff12818780c72.png"},20252:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/09_test_model-161c4e7fb04ce077fa455de10b38eb65.png"},20499:(e,n,o)=>{o.d(n,{A:()=>s});const s=o.p+"assets/images/10_test_homie-e5d4300dc883f3203ed4a7eaa7616871.png"},28453:(e,n,o)=>{o.d(n,{R:()=>d,x:()=>i});var s=o(96540);const t={},l=s.createContext(t);function d(e){const n=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);