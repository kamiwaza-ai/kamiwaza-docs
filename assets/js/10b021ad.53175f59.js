"use strict";(self.webpackChunkkamiwaza_docs=self.webpackChunkkamiwaza_docs||[]).push([[3414],{20499:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"services/serving/README","title":"Serving Service","description":"Overview","source":"@site/sdk/services/serving/README.md","sourceDirName":"services/serving","slug":"/services/serving/","permalink":"/sdk/services/serving/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"sdk","previous":{"title":"Retrieval Service","permalink":"/sdk/services/retrieval/"},"next":{"title":"VectorDB Service","permalink":"/sdk/services/vectordb/"}}');var t=l(74848),s=l(28453);const r={sidebar_position:1},d="Serving Service",o={},a=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Ray Service Management",id:"ray-service-management",level:2},{value:"Available Methods",id:"available-methods",level:3},{value:"Model Deployment",id:"model-deployment",level:2},{value:"Available Methods",id:"available-methods-1",level:3},{value:"Model Instance Management",id:"model-instance-management",level:2},{value:"Available Methods",id:"available-methods-2",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Best Practices",id:"best-practices",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"serving-service",children:"Serving Service"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["The Serving Service (",(0,t.jsx)(n.code,{children:"ServingService"}),") provides comprehensive model deployment and serving capabilities for the Kamiwaza AI Platform. Located in ",(0,t.jsx)(n.code,{children:"kamiwaza_client/services/serving.py"}),", this service manages Ray cluster operations, model deployment, and inference requests."]}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ray Service Management"}),"\n",(0,t.jsx)(n.li,{children:"Model Deployment"}),"\n",(0,t.jsx)(n.li,{children:"Model Instance Management"}),"\n",(0,t.jsx)(n.li,{children:"Model Loading/Unloading"}),"\n",(0,t.jsx)(n.li,{children:"Health Monitoring"}),"\n",(0,t.jsx)(n.li,{children:"VRAM Estimation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"ray-service-management",children:"Ray Service Management"}),"\n",(0,t.jsx)(n.h3,{id:"available-methods",children:"Available Methods"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"start_ray() -> Dict[str, Any]"}),": Initialize Ray service"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_status() -> Dict[str, Any]"}),": Get Ray cluster status"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Start Ray service\r\nstatus = client.serving.start_ray()\r\n\r\n# Check Ray status\r\nray_status = client.serving.get_status()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"model-deployment",children:"Model Deployment"}),"\n",(0,t.jsx)(n.h3,{id:"available-methods-1",children:"Available Methods"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"estimate_model_vram(model_id: UUID) -> int"}),": Estimate model VRAM requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"deploy_model(deployment: CreateModelDeployment) -> ModelDeployment"}),": Deploy a model"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"list_deployments() -> List[ModelDeployment]"}),": List all deployments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"list_active_deployments() -> List[UIModelDeployment]"}),": List only active deployments with running instances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_deployment(deployment_id: UUID) -> ModelDeployment"}),": Get deployment details"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"stop_deployment(deployment_id: UUID)"}),": Stop a deployment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_deployment_status(deployment_id: UUID) -> DeploymentStatus"}),": Get deployment status"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Estimate VRAM requirements\r\nvram_needed = client.serving.estimate_model_vram(model_id)\r\n\r\n# Deploy a model\r\ndeployment = client.serving.deploy_model(CreateModelDeployment(\r\n    model_id=model_id,\r\n    name="my-deployment",\r\n    replicas=1,\r\n    max_concurrent_requests=4\r\n))\r\n\r\n# List all deployments\r\ndeployments = client.serving.list_deployments()\r\n\r\n# List only active deployments (deployed status with running instances)\r\nactive_deployments = client.serving.list_active_deployments()\r\n# Each active deployment will have:\r\n# - id: The deployment ID\r\n# - m_id: The model ID\r\n# - m_name: The model name\r\n# - status: The deployment status\r\n# - instances: List of running instances\r\n# - lb_port: The load balancer port\r\n# - endpoint: The HTTP endpoint for the deployment (e.g. http://hostname:port/v1)\r\n\r\n# Get deployment status\r\nstatus = client.serving.get_deployment_status(deployment_id)\r\n\r\n# Stop deployment\r\nclient.serving.stop_deployment(deployment_id)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"model-instance-management",children:"Model Instance Management"}),"\n",(0,t.jsx)(n.h3,{id:"available-methods-2",children:"Available Methods"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"list_model_instances() -> List[ModelInstance]"}),": List all model instances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_model_instance(instance_id: UUID) -> ModelInstance"}),": Get instance details"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_health(deployment_id: UUID) -> Dict[str, Any]"}),": Get deployment health"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"unload_model(deployment_id: UUID)"}),": Unload model from memory"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"load_model(deployment_id: UUID)"}),": Load model into memory"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# List model instances\r\ninstances = client.serving.list_model_instances()\r\n\r\n# Get instance details\r\ninstance = client.serving.get_model_instance(instance_id)\r\n\r\n# Check deployment health\r\nhealth = client.serving.get_health(deployment_id)\r\n\r\n# Load/Unload model\r\nclient.serving.unload_model(deployment_id)\r\nclient.serving.load_model(deployment_id)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,t.jsx)(n.p,{children:"The service includes built-in error handling for common scenarios:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'try:\r\n    deployment = client.serving.deploy_model(deployment_config)\r\nexcept DeploymentError as e:\r\n    print(f"Deployment failed: {e}")\r\nexcept ResourceError as e:\r\n    print(f"Resource allocation failed: {e}")\r\nexcept APIError as e:\r\n    print(f"Operation failed: {e}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Always estimate VRAM requirements before deployment"}),"\n",(0,t.jsx)(n.li,{children:"Monitor deployment health regularly"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate number of replicas based on load"}),"\n",(0,t.jsx)(n.li,{children:"Implement proper error handling"}),"\n",(0,t.jsx)(n.li,{children:"Clean up unused deployments"}),"\n",(0,t.jsx)(n.li,{children:"Consider using advanced generation parameters for better control"}),"\n",(0,t.jsx)(n.li,{children:"Load/unload models to manage memory efficiently"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>d});var i=l(96540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);