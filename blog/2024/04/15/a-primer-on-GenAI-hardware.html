<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Harnessing the Power of Large Language Models: A Hardware Primer | Kamiwaza Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Harnessing the Power of Large Language Models: A Hardware Primer | Kamiwaza Docs"><meta data-rh="true" name="description" content="Welcome to an illuminating journey through the world of hardware essentials for deploying and leveraging Large Language Models (LLMs). In this era of generative AI, the right hardware is not just an operational need; it’s a strategic asset that can drastically elevate the performance, scalability, and efficiency of your AI applications. Whether you are integrating AI into your enterprise solutions or enhancing existing applications, understanding the interplay between hardware and AI capabilities is fundamental. Let&#x27;s dive into the crucial hardware considerations that can help you kickstart and scale your AI initiatives effectively."><meta data-rh="true" property="og:description" content="Welcome to an illuminating journey through the world of hardware essentials for deploying and leveraging Large Language Models (LLMs). In this era of generative AI, the right hardware is not just an operational need; it’s a strategic asset that can drastically elevate the performance, scalability, and efficiency of your AI applications. Whether you are integrating AI into your enterprise solutions or enhancing existing applications, understanding the interplay between hardware and AI capabilities is fundamental. Let&#x27;s dive into the crucial hardware considerations that can help you kickstart and scale your AI initiatives effectively."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-04-15T00:00:00.000Z"><link data-rh="true" rel="icon" href="/kamiwaza-docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware"><link data-rh="true" rel="alternate" href="https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware" hreflang="en"><link data-rh="true" rel="alternate" href="https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware","mainEntityOfPage":"https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware","url":"https://kamiwaza-ai.github.io/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware","headline":"Harnessing the Power of Large Language Models: A Hardware Primer","name":"Harnessing the Power of Large Language Models: A Hardware Primer","description":"Welcome to an illuminating journey through the world of hardware essentials for deploying and leveraging Large Language Models (LLMs). In this era of generative AI, the right hardware is not just an operational need; it’s a strategic asset that can drastically elevate the performance, scalability, and efficiency of your AI applications. Whether you are integrating AI into your enterprise solutions or enhancing existing applications, understanding the interplay between hardware and AI capabilities is fundamental. Let's dive into the crucial hardware considerations that can help you kickstart and scale your AI initiatives effectively.","datePublished":"2024-04-15T00:00:00.000Z","author":[],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://kamiwaza-ai.github.io/kamiwaza-docs/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/kamiwaza-docs/blog/rss.xml" title="Kamiwaza Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/kamiwaza-docs/blog/atom.xml" title="Kamiwaza Docs Atom Feed"><link rel="stylesheet" href="/kamiwaza-docs/assets/css/styles.89c1654d.css">
<script src="/kamiwaza-docs/assets/js/runtime~main.625cb3ed.js" defer="defer"></script>
<script src="/kamiwaza-docs/assets/js/main.6736b022.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/kamiwaza-docs/"><div class="navbar__logo"><img src="/kamiwaza-docs/img/logo.svg" alt="Kamiwaza Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/kamiwaza-docs/img/logo.svg" alt="Kamiwaza Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Kamiwaza Docs</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/kamiwaza-docs/">Docs</a><a class="navbar__item navbar__link" href="/kamiwaza-docs/sdk/intro">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/kamiwaza-docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item navbar__version">Version: 0.3.2</div><a href="https://github.com/kamiwaza-ai/kamiwaza-docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/kamiwaza-docs/blog/2024/04/15/a-primer-on-GenAI-hardware">Harnessing the Power of Large Language Models: A Hardware Primer</a></li></ul></div></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">Harnessing the Power of Large Language Models: A Hardware Primer</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-04-15T00:00:00.000Z">April 15, 2024</time> · <!-- -->12 min read</div></header><div id="__blog-post-container" class="markdown"><p>Welcome to an illuminating journey through the world of hardware essentials for deploying and leveraging Large Language Models (LLMs). In this era of generative AI, the right hardware is not just an operational need; it’s a strategic asset that can drastically elevate the performance, scalability, and efficiency of your AI applications. Whether you are integrating AI into your enterprise solutions or enhancing existing applications, understanding the interplay between hardware and AI capabilities is fundamental. Let&#x27;s dive into the crucial hardware considerations that can help you kickstart and scale your AI initiatives effectively.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-right-hardware-for-the-right-task-starting-your-ai-journey">The Right Hardware for the Right Task: Starting Your AI Journey<a href="#the-right-hardware-for-the-right-task-starting-your-ai-journey" class="hash-link" aria-label="Direct link to The Right Hardware for the Right Task: Starting Your AI Journey" title="Direct link to The Right Hardware for the Right Task: Starting Your AI Journey">​</a></h2>
<p>Before diving into specifics, it&#x27;s essential to understand why selecting the right hardware is critical for proving the value of AI in your business. The hardware you choose impacts everything from development speed to operational efficiency and scalability. It serves as the launching pad for your AI projects, setting the tone for innovation and performance outcomes.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nvidia-with-linux-the-leading-choice-for-ai">Nvidia with Linux: The Leading Choice for AI<a href="#nvidia-with-linux-the-leading-choice-for-ai" class="hash-link" aria-label="Direct link to Nvidia with Linux: The Leading Choice for AI" title="Direct link to Nvidia with Linux: The Leading Choice for AI">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimal-for-high-performance-ai-tasks">Optimal for High-Performance AI Tasks<a href="#optimal-for-high-performance-ai-tasks" class="hash-link" aria-label="Direct link to Optimal for High-Performance AI Tasks" title="Direct link to Optimal for High-Performance AI Tasks">​</a></h4>
<p>Nvidia GPUs, running on a Linux operating system, are a common sight in the world of AI hardware, largely due to Nvidia&#x27;s significant market share and proven track record in AI applications. This combination is highly scalable, perfect for server setups where expansion is anticipated, and offers robust AI support with superior CUDA support for intensive computations.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="considerations">Considerations<a href="#considerations" class="hash-link" aria-label="Direct link to Considerations" title="Direct link to Considerations">​</a></h4>
<ul>
<li><strong>Driver Compatibility:</strong> Linux requires careful management of GPU drivers and CUDA versions, which can be challenging in rapidly evolving setups.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apple-osx-with-metal-seamless-integration-for-mac-developers">Apple OSX with Metal: Seamless Integration for Mac Developers<a href="#apple-osx-with-metal-seamless-integration-for-mac-developers" class="hash-link" aria-label="Direct link to Apple OSX with Metal: Seamless Integration for Mac Developers" title="Direct link to Apple OSX with Metal: Seamless Integration for Mac Developers">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="best-for-integrated-apple-ecosystems">Best for Integrated Apple Ecosystems<a href="#best-for-integrated-apple-ecosystems" class="hash-link" aria-label="Direct link to Best for Integrated Apple Ecosystems" title="Direct link to Best for Integrated Apple Ecosystems">​</a></h4>
<p>For developers already embedded in the Apple ecosystem, OSX with Metal provides a streamlined performance pathway. Metal API is tailored to optimize AI operations specifically on Apple hardware, leveraging shared memory capabilities and accommodating large RAM sizes which are integral for complex AI tasks.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="considerations-1">Considerations<a href="#considerations-1" class="hash-link" aria-label="Direct link to Considerations" title="Direct link to Considerations">​</a></h4>
<ul>
<li><strong>Limited GPU Support:</strong> Apple hardware typically does not support external GPUs as flexibly as PC setups, limiting performance scalability.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="windows-environments">Windows Environments<a href="#windows-environments" class="hash-link" aria-label="Direct link to Windows Environments" title="Direct link to Windows Environments">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="versatile-but-with-nuanced-support">Versatile but with Nuanced Support<a href="#versatile-but-with-nuanced-support" class="hash-link" aria-label="Direct link to Versatile but with Nuanced Support" title="Direct link to Versatile but with Nuanced Support">​</a></h4>
<p>Windows has long been a first class citizen for nVidia drivers due to its heritage in gaming. Indeed, nVidia has even release a <a href="https://blogs.nvidia.com/blog/chat-with-rtx-available-now/" target="_blank" rel="noopener noreferrer">Chat With RTX</a> local LLM/RAG app; the problem with Windows for Enterprise is that it can be much more challenging to see the direct path between development and production due to the large OS deltas. One reason Kamiwaza opted to build and support OSX hand-in-hand with Linux as a production environment was that the support on OSX was nearly like-for-like, providing a very consistent development -&gt; production experience.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-model-scales-from-slm-to-llm">Understanding Model Scales: From SLM to LLM<a href="#understanding-model-scales-from-slm-to-llm" class="hash-link" aria-label="Direct link to Understanding Model Scales: From SLM to LLM" title="Direct link to Understanding Model Scales: From SLM to LLM">​</a></h2>
<p>Before discussing specific models, it&#x27;s crucial to understand what a &quot;model&quot; in AI parlance means. Essentially, the term refers to the underlying AI that has been trained to understand and generate human-like text based on the data it has been fed. Models are measured in parameters—a parameter being a piece of the model that has learned a specific part of the data during training. The scale of the model, from Small Language Models (SLMs) like Google&#x27;s Gemma-2B, to an optimized open source 7B model like <a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" target="_blank" rel="noopener noreferrer">OpenHermes-2.5-Mistral-7B</a> for drafting emails, to Large Language Models (LLMs) like HuggingFace’s Zephyr-Orpo 141B, a fine-tuning of Mistral Mixtral-8x22B models.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="small-language-models-slm--developer-systems">Small Language Models (SLM) &amp; Developer Systems<a href="#small-language-models-slm--developer-systems" class="hash-link" aria-label="Direct link to Small Language Models (SLM) &amp; Developer Systems" title="Direct link to Small Language Models (SLM) &amp; Developer Systems">​</a></h3>
<p><strong>Hardware Needs:</strong> Can run efficiently on consumer-grade laptops or desktops typically; and even, depending on the hardware, directly on consumer mobile devices. Qualcomm announced in early 2023 a working version of Stable Diffusion <a href="https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android" target="_blank" rel="noopener noreferrer">on an Android phone</a>. That said, for prototyping we recommend either Macbook M* with ample RAM (the M2MAX/M3 Macbook Pros, or equivalent Mac desktops, are extremely popular with developers and other professionals, but with heavy RAM, 96-128GB, they can run extremely large models).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-llm">Large Language Models (LLM)<a href="#large-language-models-llm" class="hash-link" aria-label="Direct link to Large Language Models (LLM)" title="Direct link to Large Language Models (LLM)">​</a></h3>
<p>For larger models, heavy production use, you typically now want to turn to production-grade hardware. While the typical choice today is Linux servers with nVidia GPUs, there are a lot of alternatives here, including:</p>
<ul>
<li><strong>CPU-based inference</strong> on Intel AVX512-capable chips</li>
<li><strong>Qualcomm</strong> <a href="https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100" target="_blank" rel="noopener noreferrer">AI 100</a> series, with their AI 100 Pro and AI 100 Ultra, which are extremely powerful hardware with impressively low power consumption</li>
<li><strong>AMD</strong> <a href="https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html" target="_blank" rel="noopener noreferrer">AMD Instinct</a> is AMD&#x27;s answer to nVidia&#x27;s dominance and sport impressively large VRAM and solid performance; but many caution around software support in newer libraries</li>
<li><strong>nVidia</strong> nVidia needs no introduction; their A100 and H100, with their upcoming GH200 and the recently announced <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing" target="_blank" rel="noopener noreferrer">Blackwell</a> GPUs starting with the B100. nVidia has reaped the boom of GenAI very notably with orgs like Meta announcing a buy of <a href="https://www.pcmag.com/news/zuckerbergs-meta-is-spending-billions-to-buy-350000-nvidia-h100-gpus" target="_blank" rel="noopener noreferrer">350,000 H100s</a>, adding to an extremely large farm of GPUs already operated - this helps them attract and retain talent developing their in-house open-sourced LLMs, like <a href="https://llama.meta.com/llama2/" target="_blank" rel="noopener noreferrer">Llama 2</a>, arguably the most influential open-source AI model ever released.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-performance-metrics-token-speed-and-vram">Decoding Performance Metrics: Token Speed and VRAM<a href="#decoding-performance-metrics-token-speed-and-vram" class="hash-link" aria-label="Direct link to Decoding Performance Metrics: Token Speed and VRAM" title="Direct link to Decoding Performance Metrics: Token Speed and VRAM">​</a></h2>
<p>When selecting hardware for AI, understanding performance metrics is crucial. Token speed and VRAM stand out as primary indicators.</p>
<p><strong>Tokens/second</strong> is just a metric of how fast a given model engine can generate output tokens. For a visual in-context view on model deployment and stress test, check out the <a href="https://www.youtube.com/watch?v=h4yyqfw9liY" target="_blank" rel="noopener noreferrer">Kamiwaza Model Deployment and Inference Stress Test</a> video on YouTube.</p>
<p><img decoding="async" loading="lazy" alt="Tokens Per Second CLI readout" src="/kamiwaza-docs/assets/images/blog_tokens_sec-0c9e365f52edc8c6855b0e73e0b620dd.png" width="968" height="119" class="img_ev3q"></p>
<p>It&#x27;s important to note that due to the way the hardware and software interact, there&#x27;s a difference on many hardware platforms between the speed you would see in a single client generation (e.g., one prompt -&gt; one response) vs batched. For example, in the image above, our stress test averaged almost 1200 tokens/second but the same card responding to a single request would likely only get ~40-60 tokens/second at best; but it can perform many parallel inferences when batching. Kamiwaza helps with model deployment to make this easier to manage.</p>
<ul>
<li><strong>Model Size Impact:</strong> Larger models, while capable of generating more complex texts, tend to operate more slowly due to their size. More <strong>parameters</strong> means more math operations for each pass through a model. You can think of it as a set of multiplication operations through dense matrices, each full pass popping out the next generated token based on the input context and probabilities in the neural network weights.</li>
<li><strong>VRAM Requirements</strong> As a rule, you can think of memory requirements being driven by three things:</li>
</ul>
<ol>
<li><strong>The number of Parameters</strong>: For a &quot;70B&quot; model, 70 billion parameters means 70 billion numbers in the neural network, effectively. Model models at &quot;full weight&quot; for inference at 16-bit, and you may see the term <em>float16</em> or <em>bfloat16</em> which are specific 2-byte data types. So a full-weight 70B model needs 140GB of memory to hold the weights. This number can be reduced through using a lower-precision model, through a technique called <strong>quantization</strong> which is essentially a neural-network specific version of &quot;rounding&quot; the numbers; so instead of a 16-bit number, each weight becomes an 8-bit or a 4-bit weight. (And quantization has more variety than that, such as quantizing different layers to different weights, as some are more performance-sensitive to that rounding)</li>
<li><strong>The context</strong>: The user input must be converted into numbers also, called an embedding, to be processed by the network; there is the direct usage of the embedding, but there is also intermediate calculations and caches; the usage of these varies by architecture.  There are some tools like this <a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator" target="_blank" rel="noopener noreferrer">VRAM calculator on HuggingFace</a> that can help estimate; something like 1-2GB per 4096 input context tokens at a batch size of 512 isn&#x27;t a terrible rule of thumb, but this varies quite a bit.</li>
<li><strong>Added KV Cache for Performance</strong>: Engines like vLLM (the default deployment engine Kamiwaza deploys on when using Linux) pre-allocate additional memory for KV cache; this can dramatically speed up performance. vLLM will happily pre-allocate an entire 80GB card even on a 7B parameter model (so, one you could definitely run easily at full weight even on a 24GB consumer card), but it doesn&#x27;t need that much. Kamiwaza&#x27;s default deployment configs can be modified but try to allocate a recommended amount of memory by default for a performance boost, without being excessively generous.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="advancements-in-model-efficiency">Advancements in Model Efficiency<a href="#advancements-in-model-efficiency" class="hash-link" aria-label="Direct link to Advancements in Model Efficiency" title="Direct link to Advancements in Model Efficiency">​</a></h2>
<p>In the dynamic landscape of AI, continuous improvements are crucial for enhancing model performance and managing resource utilization effectively. Key techniques have emerged that not only boost computational efficiency but also optimize the overall functioning of AI models:</p>
<ul>
<li><strong>Quantization:</strong> This technique transforms the model to operate on lower precision (e.g., 8-bit integers instead of 16-bit floats), which can significantly increase the speed of computations and reduce the overall model size. This is particularly beneficial in environments with limited hardware capabilities or where rapid response times are crucial.</li>
<li><strong>Mixture of Depths:</strong> Adapting the depth of the neural network layers according to the task complexity can optimize processing time and power consumption. This approach tailors the resource allocation based on the immediate needs of the application, ensuring efficient use of computational power.</li>
<li><strong>Dynamic Pruning:</strong> By temporarily reducing the size of the neural network during computations, dynamic pruning helps conserve resources without a notable compromise in performance. This method is particularly useful in runtime environments where flexibility in resource allocation can lead to cost efficiencies.</li>
</ul>
<p>These techniques represent the forefront of making large-scale models more accessible and manageable, paving the way for broader adoption across various industries.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="comprehensive-approach-to-ai-application-stack">Comprehensive Approach to AI Application Stack<a href="#comprehensive-approach-to-ai-application-stack" class="hash-link" aria-label="Direct link to Comprehensive Approach to AI Application Stack" title="Direct link to Comprehensive Approach to AI Application Stack">​</a></h2>
<p>Implementing LLMs effectively requires a comprehensive understanding and integration of multiple components of the AI application stack. This holistic approach ensures that each layer is optimized for maximum performance and efficiency:</p>
<ol>
<li><strong>Hardware:</strong> The foundational layer, which includes high-performance GPUs and CPUs, tailored to the needs of demanding AI tasks.</li>
<li><strong>Operating System:</strong> Choosing the right OS—whether it be Linux, OSX, or Windows—is crucial as it must synergize with the hardware to optimize performance and provide stability.</li>
<li><strong>Model Engine:</strong> Utilizing advanced frameworks like TensorFlow or PyTorch, which are designed to leverage the underlying hardware capabilities to the fullest.</li>
<li><strong>Application-Level Packages:</strong> Tools such as LangChain and dSPY provide specialized functionalities that are essential for developing sophisticated AI applications. They serve as the building blocks for creating user-centric solutions that harness the power of LLMs.</li>
</ol>
<p>With a solid grasp of these components, organizations can navigate the complexities of AI deployment more confidently, ensuring that their AI initiatives are both scalable and robust.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bottom-line-recommendations">Bottom Line Recommendations<a href="#bottom-line-recommendations" class="hash-link" aria-label="Direct link to Bottom Line Recommendations" title="Direct link to Bottom Line Recommendations">​</a></h2>
<p>These are &quot;inference only&quot; recommendations; for training, there may be other considerations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nvidia">nVidia<a href="#nvidia" class="hash-link" aria-label="Direct link to nVidia" title="Direct link to nVidia">​</a></h3>
<ul>
<li>At least 3 hosts for redundancy for Enterprise</li>
<li>Homogenous hardware config for cpu/gpu/memory; Kamiwaza can deploy multiple models on larger cards</li>
<li><strong>RTX4090</strong> is often at performance parity with the datacenter A100 card for inference; they have 24GB of memory per card
-- You can find deployment options fairly standard for configs of up to 6x4090, giving 192GB of VRAM
-- Being a consumer card, the RTX4090 can typically be found at ~$2k/card, meaning the 6x setup is favorable vs a single A100 datacenter card</li>
<li><strong>A100, H100</strong> are the kings of the &quot;available&quot; datacenter-class hardware
-- You can build single hosts at up to 8x
-- They come in PCEe and SXM form factors
-- Both very widely available in cloud instances
-- 40GB and 80GB VRAM versions
-- Relatively expensive hardware (as of this writing, a single A100 80GB is still about $17,000, just for 1 card); although prices have been coming down lately
-- For the most powerful open models, 4x or more can be required. For example, the recently released Apache-licensed <a href="https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1" target="_blank" rel="noopener noreferrer">Mixtral-8x22B-v0.1 model</a> requires <strong>~260GB</strong> of VRAM to load the full-weight (non-quanitzied) version, plus memory for kv cache, and input context; this means this model can largely consume a <strong>4xA100-80B</strong> setup on its own. Commensurately, however, fine-tunes such as the <a href="https://huggingface.co/microsoft/WizardLM-2-8x22B" target="_blank" rel="noopener noreferrer">Microsoft WizardLM fine-tune</a> are stronger than the original GPT4 model, which combined with its 64K context length makes it comparable to being able to operate a completely private GPT4-class model
-- In the middle ground, many organizations will find that base or fine-tuned version of models like <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" target="_blank" rel="noopener noreferrer">Mixtral-8x7B</a> or <a href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct" target="_blank" rel="noopener noreferrer">DeepSeek-Coder-33B</a> will be appropriate; possibly at any weights from the full weight (using a single A100) to a strong 4-5 bit quantization (which can run on a single RTX4090, or locally on a well-equippted mac/macbook pro).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-nvidia-hardware">Other nVidia Hardware<a href="#other-nvidia-hardware" class="hash-link" aria-label="Direct link to Other nVidia Hardware" title="Direct link to Other nVidia Hardware">​</a></h3>
<p>As a footnote, there are a number of other cards that share similar architectures; for example, the nVidia Ada architecture powers the RTX4xxx series, but also the &quot;pro&quot; cards, like the RTX6000, as well the L40, sport (up to) 48GB of memory. Those can be reasonable choices as well.</p>
<p>For the curious, the highest-end cards, like the A100 and H100, excel at 32-bit workloads, which is why classic model training happens almost exclusively on them, as they offer the mix of large memory and good performance at high precision; this is an extremely different workload than run of the mill inference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-vs-owned-and-operated">Cloud vs Owned and Operated<a href="#cloud-vs-owned-and-operated" class="hash-link" aria-label="Direct link to Cloud vs Owned and Operated" title="Direct link to Cloud vs Owned and Operated">​</a></h2>
<p>We find our customers anywhere on the spectrum of:</p>
<ul>
<li>Open to Cloud, but want to control the stack: fine to use cloud instances, and Kamiwaza is cloud-deployable easily; in fact, we have a fully-scripted install that is tested on Microsoft Azure for releases, against Ubuntu 22.04LTS-Server</li>
<li>Cloud ok for Test-Dev: They have use cases they want running on owned/operated hardware, but they will test on cloud instances</li>
<li>Owned &amp; Operated only: The enterprise wants to keep their entire flow private on hardware and software they control, for a variety of reasons</li>
</ul>
<p>Obviously cloud, even using spot instances, can be a good way to do some early testing on certain models or use cases without a purchase.</p></div></article></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-right-hardware-for-the-right-task-starting-your-ai-journey" class="table-of-contents__link toc-highlight">The Right Hardware for the Right Task: Starting Your AI Journey</a><ul><li><a href="#nvidia-with-linux-the-leading-choice-for-ai" class="table-of-contents__link toc-highlight">Nvidia with Linux: The Leading Choice for AI</a></li><li><a href="#apple-osx-with-metal-seamless-integration-for-mac-developers" class="table-of-contents__link toc-highlight">Apple OSX with Metal: Seamless Integration for Mac Developers</a></li><li><a href="#windows-environments" class="table-of-contents__link toc-highlight">Windows Environments</a></li></ul></li><li><a href="#understanding-model-scales-from-slm-to-llm" class="table-of-contents__link toc-highlight">Understanding Model Scales: From SLM to LLM</a><ul><li><a href="#small-language-models-slm--developer-systems" class="table-of-contents__link toc-highlight">Small Language Models (SLM) &amp; Developer Systems</a></li><li><a href="#large-language-models-llm" class="table-of-contents__link toc-highlight">Large Language Models (LLM)</a></li></ul></li><li><a href="#decoding-performance-metrics-token-speed-and-vram" class="table-of-contents__link toc-highlight">Decoding Performance Metrics: Token Speed and VRAM</a></li><li><a href="#advancements-in-model-efficiency" class="table-of-contents__link toc-highlight">Advancements in Model Efficiency</a></li><li><a href="#comprehensive-approach-to-ai-application-stack" class="table-of-contents__link toc-highlight">Comprehensive Approach to AI Application Stack</a></li><li><a href="#bottom-line-recommendations" class="table-of-contents__link toc-highlight">Bottom Line Recommendations</a><ul><li><a href="#nvidia" class="table-of-contents__link toc-highlight">nVidia</a></li><li><a href="#other-nvidia-hardware" class="table-of-contents__link toc-highlight">Other nVidia Hardware</a></li></ul></li><li><a href="#cloud-vs-owned-and-operated" class="table-of-contents__link toc-highlight">Cloud vs Owned and Operated</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/kamiwaza-docs/">Platform</a></li><li class="footer__item"><a class="footer__link-item" href="/kamiwaza-docs/sdk/intro">SDK</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/kamiwaza-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discord.gg/cVGBS5rD2U" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Kamiwaza AI.</div></div></div></footer></div>
</body>
</html>